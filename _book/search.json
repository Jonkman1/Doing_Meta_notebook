[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notitieboekje dat hoort bij Doing Meta-Analysis with R. A Hands-On Guide",
    "section": "",
    "text": "Voorwoord\nDit is mijn notitieboekje dat hoort bij Doing Meta-Analysis with R dat Mathias Harrer, Pim Cuijpers, Toshi Furukawa, and David Ebert schreven. Dat boek van Harrer en anderen is een open source boek dat je kunt vinden op https://doingmetaanalysis.github.io. Omdat ik de komende maanden een groep studenten begeleid bij het maken van een masterthesis, waarbij ze de techniek van meta-analyse gebruiken, heb ik dit gemaakt om hen goed bij het uitvoeren van hun activiteiten te ondersteunen.\n\n\n\nHet boek"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "01-introductie.html",
    "href": "01-introductie.html",
    "title": "1  Introductie",
    "section": "",
    "text": "Met die grote hoeveelheden onderzoeksresultaten die elke dag worden geproduceerd, is het nog belangrijker geworden om het bewijsmateriaal in zijn geheel te bekijken en het kritisch te beoordelen. Meta-analyse kan hierbij enorm helpen, zolang de beperkingen en vooroordelen ervan worden erkend. Het doel van meta-analyse is om al het beschikbare bewijsmateriaal met betrekking tot een duidelijk gedefinieerd onderzoeksgebied of onderzoeksvraag te combineren, dat samen te vatten en uiteindelijk te interpreteren. Er zijn ten minste drie verschillende manieren waarop bewijs uit meerdere onderzoeken kan worden samengevoegd:\n\nTraditionele/narratieve reviews (geschreven door experts en autoriteiten in het veld zonder duidelijke regels).\n\nSystematische reviews (om bewijs samen te vatten met behulp van duidelijk gedefinieerde en transparante regels).\n\nMeta-analyses (om bewijs te combineren en samen te vatten over alle onderzoeken met behulp van duidelijk gedefinieerde en transparante regels op kwantitatieve wijze).\n\n\nMeta-analyse is niet uitgevonden door één persoon alleen, maar kent vele grondleggers en vaders. De eerste pogingen om de effecten van afzonderlijke, maar vergelijkbare onderzoeken statistisch samen te vatten, dateren van ongeveer 100 jaar geleden. Talloze innovaties hebben de toepasbaarheid, robuustheid en veelzijdigheid van de meta-analytische methoden in de laatste vier decennia vergroot. In die periode is meta-analyse een universeel geaccepteerd onderzoeksinstrument geworden. Er zijn veel valkuilen en het boek leidt ons langs veelvoorkomende valkuilen in de uitvoering van meta-analyse. Hier worden er vier genoemd:\n\n\nHet “Appels en Sinaasappelen” probleem: Je zou kunnen stellen dat meta-analyse appels met peren combineren is. De reikwijdte en specificiteit van een meta-analyse moet daarom gebaseerd zijn op de onderzoeksvraag die beantwoord moet worden, en deze vraag moet van praktisch belang zijn. Variatie tussen studies kan onproblematisch zijn, zelfs inzichtgevend, als die op de juiste manier wordt opgenomen in de doelen en probleemspecificatie van een meta-analyse.\n\nHet “Vuiligheid Erin, Vuiligheid Eruit”-probleem: De kwaliteit van het bewijs dat een meta-analyse oplevert, hangt sterk af van de kwaliteit van de studies die erin worden samengevat. Anders krijg je “vuiligheid erin, vuiligheid eruit”.\n\nHet “Dossierlader”-probleem: Het dossierladerprobleem verwijst naar het probleem dat niet alle relevante onderzoeksresultaten worden gepubliceerd. Negatieve of ‘teleurstellende’ resultaten zijn ondervertegenwoordigd. Daarom moet je kijken naar publicatiebias.\n\nHet “agenda van de onderzoeker”-probleem: Meta-analyse gaat gepaard met veel “vrijheidsgraden van de onderzoeker”, waardoor er veel ruimte overblijft voor beslissingen die soms willekeurig en soms het resultaat van geheime persoonlijke voorkeuren zijn. Een manier om het probleem van de agenda van de onderzoeker te verminderen, is het vooraf registreren en publiceren van een gedetailleerd analyseplan voordat met het verzamelen van gegevens voor een meta-analyse wordt begonnen.\n\n\nBelangrijk is dat de methodologische beslissingen zowel transparant als reproduceerbaar zijn. Hier worden de eerste bouwstenen gepresenteerd. Definieer de onderzoeksvraag: of een goede onderzoeksvraag te definiëren, helpt het om deze eerst te zien als een vorm van probleemspecificatie. Door jezelf stap voor stap deze vragen te stellen, zou het gemakkelijker moeten worden om te definiëren wat je wilt bereiken met je meta-analyse. Gebruik bijvoorbeeld het FINER-raamwerk (Feasible, Interesting, Novel, Ethical, Relevant) of PICO (Population, Intervention, Comparison, Outcome) om je onderzoeksvraag te definiëren.\nKijk verder ook naar in aanmerking komende onderzoeksdesigns, houd rekening met culturele en taalkundige verschillen tussen in aanmerking komende studies, en kijk naar publicatietype en de rol van grijze literatuur. Schrijf deze dingen in inclusie en uitsluitingscriteria. Houd bij dit alles rekening met de PRISMA- () en MARS- ()-richtlijnen. Veel van deze problemen kunnen worden verminderd door een duidelijke onderzoeksvraag en geschiktheidscriteria te definiëren, een analyseplan te schrijven, de meta-analyse vooraf te registreren en de studiezoektocht en gegevensextractie op een systematische en reproduceerbare manier uit te voeren."
  },
  {
    "objectID": "02-discovering_R.html",
    "href": "02-discovering_R.html",
    "title": "2  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "03-effect_sizes.html",
    "href": "03-effect_sizes.html",
    "title": "3  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "04-pooling_effect_sizes.html",
    "href": "04-pooling_effect_sizes.html",
    "title": "4  Effect Sizes samenbrengen",
    "section": "",
    "text": "Theory\nIn de statistiek wordt een model gezien als een vereenvoudigde “theorie”, die het proces beschrijft waarmee de waargenomen gegevens zijn gegenereerd. Vaak wordt het geschreven als een wiskundige formule om processen in de wereld om ons heen op een geïdealiseerde manier te beschrijven.\nEr zijn twee alternatieve modellen in de meta-analyse: het fixed-effect model en het random-effects model. Het fixed-effect model gaat ervan uit dat er één ware effectgrootte is en dat alle effectgroottes afkomstig zijn van één homogene populatie.\nHet random-effects model daarentegen stelt dat de ware effectgroottes ook variëren binnen meta-analyses. Het anticipeert op aanzienlijke heterogeniteit tussen studies in de ware effecten. Het doel van het random-effects model is daarom om het gemiddelde te vinden van de ware effectgrootteverdeling die ten grondslag ligt aan onze gegevens. De variantie van de ware effectgroottes \\(\\tau2\\), ook wel heterogeniteitsvariantie tussen studies genoemd, moet worden geschat in random-effects meta-analyses. Hier zijn verschillende methoden voor, en welke het beste werkt hangt af van de context.\nDe meest gebruikelijke manier om een gepoolde effectgrootte te berekenen is via de inverse-variantiemethode. In het meta-pakket is er een functie om meta-analyses uit te voeren van vooraf berekende effectgroottegegevens, evenals een reeks functies die kunnen worden gebruikt voor verschillende soorten “ruwe” uitkomstgegevens.\nPractijk\nEerst kijken we hoe we een fixed-model kunnen uitvoeren met het esc pakket:\n\n# Laad dmetar, esc en tidyverse\nlibrary(dmetar)    ## voor de data\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(esc)       ## voor de berekening van effect groottes \nlibrary(tidyverse) ## voor databewerking\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Laad de dataset via dmetar\ndata(SuicidePrevention)\n\n# Bekijk de dataset en zie welke variabelen (kolommen) en studies (rijen) er zijn\nglimpse(SuicidePrevention)\n\nRows: 9\nColumns: 10\n$ author    &lt;chr&gt; \"Berry et al.\", \"DeVries et al.\", \"Fleming et al.\", \"Hunt & …\n$ n.e       &lt;dbl&gt; 90, 77, 30, 64, 50, 109, 60, 40, 51\n$ mean.e    &lt;dbl&gt; 14.98, 16.21, 3.01, 19.32, 4.54, 15.11, 3.44, 7.10, 23.74\n$ sd.e      &lt;dbl&gt; 3.29, 5.35, 0.87, 6.41, 2.75, 4.63, 1.26, 0.76, 7.24\n$ n.c       &lt;dbl&gt; 95, 69, 30, 65, 50, 111, 60, 40, 56\n$ mean.c    &lt;dbl&gt; 15.54, 20.13, 3.13, 20.22, 5.61, 16.46, 3.42, 7.38, 24.91\n$ sd.c      &lt;dbl&gt; 4.41, 7.43, 1.23, 7.62, 2.66, 5.39, 1.88, 1.41, 10.65\n$ pubyear   &lt;dbl&gt; 2006, 2019, 2006, 2011, 1997, 2000, 2013, 2015, 2014\n$ age_group &lt;fct&gt; gen, older, gen, gen, gen, gen, gen, older, older\n$ control   &lt;fct&gt; WLC, no intervention, no intervention, WLC, WLC, no interven…\n\n\nAls we dat weten kunnen we de dataset analyseren:\n\n# Bereken Hedges' g en de Standaard Fout (SE)\n# We slaan de studienamen op in \"study\".\n# We gebruiken de pmap_dfr functie om de effect grootte te berekenen voor elke rij studie.\n# Kijk goed wat we gebruiken\nSP_calc &lt;- pmap_dfr(SuicidePrevention, \n                    function(mean.e, sd.e, n.e, mean.c,\n                             sd.c, n.c, author, ...){\n                      esc_mean_sd(grp1m = mean.e,\n                                  grp1sd = sd.e,\n                                  grp1n = n.e,\n                                  grp2m = mean.c,\n                                  grp2sd = sd.c,\n                                  grp2n = n.c,\n                                  study = author,\n                                  es.type = \"g\") %&gt;% \n                        as.data.frame()}) \n\n# Laten we de nieuwe dataset opnieuw bekijken\n# Je ziet dat Hedges' g (\"es\") en standard error (\"se\") erin zit\nglimpse(SP_calc)\n\nRows: 9\nColumns: 9\n$ study       &lt;chr&gt; \"Berry et al.\", \"DeVries et al.\", \"Fleming et al.\", \"Hunt …\n$ es          &lt;dbl&gt; -0.14279447, -0.60770928, -0.11117965, -0.12698011, -0.392…\n$ weight      &lt;dbl&gt; 46.09784, 34.77314, 14.97625, 32.18243, 24.52054, 54.50431…\n$ sample.size &lt;dbl&gt; 185, 146, 60, 129, 100, 220, 120, 80, 107\n$ se          &lt;dbl&gt; 0.1472854, 0.1695813, 0.2584036, 0.1762749, 0.2019459, 0.1…\n$ var         &lt;dbl&gt; 0.02169299, 0.02875783, 0.06677240, 0.03107286, 0.04078214…\n$ ci.lo       &lt;dbl&gt; -0.4314686, -0.9400826, -0.6176413, -0.4724727, -0.7882811…\n$ ci.hi       &lt;dbl&gt; 0.145879624, -0.275335960, 0.395282029, 0.218512440, 0.003…\n$ measure     &lt;chr&gt; \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\"\n\n\nWe gebruiken deze resultaten om de formule van het fixed-effect model toe te passen:\n\n# Bereken de inverse variantie-gewichten voor elk onderzoek.\nSP_calc$w &lt;- 1/SP_calc$se^2\n\n# Vervolgens gebruiken we de gewichten om het gecombineerde effect te berekenen\npooled_effect &lt;- sum(SP_calc$w*SP_calc$es)/sum(SP_calc$w)\npooled_effect\n\n[1] -0.2311121\n\n\nDe resultaten van onze berekeningen laten zien dat de gepoolde effectgrootte, uitgaande van een model met een vast effect, \\(g \\approx -0,23\\) is. Later laten we zien hoe het random model werkt.\nmetagen kan worden gebruikt voor vooraf berekende effectgroottegegevens. Hier wordt het gebruikt om een meta-analyse uit te voeren van de ThirdWave dataset. Laten we de dataset openen en bekijken.\n\nlibrary(tidyverse) # voor databewerking\nlibrary(dmetar)    # voor de data\nlibrary(meta)      # voor de meta-analyse\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\ndata(ThirdWave)\nglimpse(ThirdWave)\n\nRows: 18\nColumns: 8\n$ Author               &lt;chr&gt; \"Call et al.\", \"Cavanagh et al.\", \"DanitzOrsillo\"…\n$ TE                   &lt;dbl&gt; 0.7091362, 0.3548641, 1.7911700, 0.1824552, 0.421…\n$ seTE                 &lt;dbl&gt; 0.2608202, 0.1963624, 0.3455692, 0.1177874, 0.144…\n$ RiskOfBias           &lt;chr&gt; \"high\", \"low\", \"high\", \"low\", \"low\", \"low\", \"high…\n$ TypeControlGroup     &lt;chr&gt; \"WLC\", \"WLC\", \"WLC\", \"no intervention\", \"informat…\n$ InterventionDuration &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"sho…\n$ InterventionType     &lt;chr&gt; \"mindfulness\", \"mindfulness\", \"ACT\", \"mindfulness…\n$ ModeOfDelivery       &lt;chr&gt; \"group\", \"online\", \"group\", \"group\", \"online\", \"g…\n\n\nNu gaan we voor de analyse het random-effect model gebruken via het pakket meta.\nTE bevat de \\(g\\) waarden en seTE de standaardfout ervan. De metagen functie neemt de volgende argumenten:\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\nOns m.gen object bevat nu alle resultaten van de meta-analyse. Een gemakkelijke manier om een overzicht te krijgen is door de summary functie te gebruiken.\n\nsummary(m.gen)\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.3\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.9\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.7\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\nHet gepoolde effect direct weergeven is ook mogelijk. Hier dat effect via random-methode\n\nm.gen$TE.random\n\n[1] 0.5771158\n\n\nHier dat effect via fixed-methode.\n\nm.gen$TE.fixed\n\n[1] 0.4805045\n\n\nJe kunt ook een sensitiviteis-analyse uitvoeren. Dan zet je een andere methode in om te zien of de resultaten dan hetzelfde zijn. We kunnen bijvoorbeeld de functie update.meta gebruiken om de methode voor het schatten van \\(\\tau^2\\) te veranderen van de standaard REML in PM (Paule-Mandel). Dit is een goed idee bijvoorbeeld als het aantal studies klein is.\n\nm.gen_update &lt;- update.meta(m.gen, \n                            method.tau = \"PM\")\n\n# effect\nm.gen_update$TE.random\n\n[1] 0.5873544\n\n\nOm de \\(\\tau^2\\) schatting te krijgen doe je:\n\nm.gen_update$tau2\n\n[1] 0.1104957\n\n\nNu iets over binaire uitkomsten. We gebruiken de functie metabin om een meta-analyse uit te voeren op de dataset DepressionMortality. Laten we de dataset openen en eerste eens bekijken.\n\nlibrary(dmetar)\nlibrary(tidyverse)\nlibrary(meta)\n\ndata(DepressionMortality)\nglimpse(DepressionMortality)\n\nRows: 18\nColumns: 6\n$ author  &lt;chr&gt; \"Aaroma et al., 1994\", \"Black et al., 1998\", \"Bruce et al., 19…\n$ event.e &lt;dbl&gt; 25, 65, 5, 26, 32, 1, 24, 15, 15, 173, 37, 41, 29, 61, 15, 21,…\n$ n.e     &lt;dbl&gt; 215, 588, 46, 67, 407, 44, 60, 61, 29, 1015, 105, 120, 258, 38…\n$ event.c &lt;dbl&gt; 171, 120, 107, 1168, 269, 87, 200, 437, 227, 250, 66, 9, 24, 3…\n$ n.c     &lt;dbl&gt; 3088, 1901, 2479, 3493, 6256, 1520, 882, 2603, 853, 3375, 409,…\n$ country &lt;chr&gt; \"Finland\", \"USA\", \"USA\", \"USA\", \"Sweden\", \"USA\", \"Canada\", \"Ne…\n\n\nIn dit voorbeeld wordt de risk ratio, RR(Risicoverhouding)) berekend (de binaire uitkomst). We gebruiken een random-effect poolingmodel en omdat we te maken hebben met binaire uitkomstgegevens, gebruiken we de Paule-Mandel-schatter voor \\(tau2\\).\n\nm.bin &lt;- metabin(event.e = event.e, \n                 n.e = n.e,\n                 event.c = event.c,\n                 n.c = n.c,\n                 studlab = author,\n                 data = DepressionMortality,\n                 sm = \"RR\",\n                 method = \"MH\",\n                 MH.exact = TRUE,\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"PM\",\n                 hakn = TRUE,\n                 title = \"Depressie en Mortaliteit\")\nsummary(m.bin)\n\nReview:     Depressie en Mortaliteit\n\n                          RR            95%-CI %W(random)\nAaroma et al., 1994   2.0998 [1.4128;  3.1208]        6.0\nBlack et al., 1998    1.7512 [1.3139;  2.3341]        6.6\nBruce et al., 1989    2.5183 [1.0785;  5.8802]        3.7\nBruce et al., 1994    1.1605 [0.8560;  1.5733]        6.5\nEnzell et al., 1984   1.8285 [1.2853;  2.6014]        6.3\nFredman et al., 1989  0.3971 [0.0566;  2.7861]        1.2\nMurphy et al., 1987   1.7640 [1.2644;  2.4610]        6.4\nPenninx et al., 1999  1.4647 [0.9361;  2.2919]        5.8\nPulska et al., 1998   1.9436 [1.3441;  2.8107]        6.2\nRoberts et al., 1990  2.3010 [1.9206;  2.7567]        7.0\nSaz et al., 1999      2.1837 [1.5533;  3.0700]        6.3\nSharma et al., 1998   2.0500 [1.0744;  3.9114]        4.7\nTakeida et al., 1997  6.9784 [4.1303; 11.7902]        5.3\nTakeida et al., 1999  5.8124 [3.8816;  8.7035]        6.0\nThomas et al., 1992   1.3303 [0.7780;  2.2745]        5.3\nThomas et al., 1992   1.7722 [1.1073;  2.8363]        5.6\nWeissman et al., 1986 1.2500 [0.6678;  2.3398]        4.8\nZheng et al., 1997    1.9803 [1.4001;  2.8011]        6.3\n\nNumber of studies: k = 18\nNumber of observations: o = 94770\nNumber of events: e = 5439\n\n                         RR           95%-CI    t  p-value\nRandom effects model 2.0217 [1.5786; 2.5892] 6.00 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.1865 [0.0739; 0.5568]; tau = 0.4319 [0.2718; 0.7462]\n I^2 = 77.2% [64.3%; 85.4%]; H = 2.09 [1.67; 2.62]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 74.49   17 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\nWe gebruiken een andere schattingsmethode om de resultaten nogmaals te controleren (sensitiviteits-analyse), REML ipv MH.\n\nm.bin_update &lt;- update.meta(m.bin, \n                            method.tau = \"REML\")\n\nDe resultaten zijn vrijwel hetzelfde:\n\nexp(m.bin_update$TE.random)\n\n[1] 2.02365\n\n\nHier de schatting voor \\(\\tau^2\\):\n\nm.bin_update$tau2\n\n[1] 0.1647315\n\n\nLaten we daarvoor sm overzetten naar OR:\n\nm.bin_or &lt;- metabin(event.e = event.e, \n                 n.e = n.e,\n                 event.c = event.c,\n                 n.c = n.c,\n                 studlab = author,\n                 data = DepressionMortality,\n                 sm = \"OR\",\n                 method = \"MH\",\n                 MH.exact = TRUE,\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"PM\",\n                 hakn = TRUE,\n                 title = \"Depressie en Mortaliteit\")\nsummary(m.bin_or)\n\nReview:     Depressie en Mortaliteit\n\n                          OR            95%-CI %W(random)\nAaroma et al., 1994   2.2445 [1.4389;  3.5011]        6.3\nBlack et al., 1998    1.8446 [1.3432;  2.5331]        7.0\nBruce et al., 1989    2.7034 [1.0472;  6.9793]        3.7\nBruce et al., 1994    1.2623 [0.7684;  2.0737]        6.0\nEnzell et al., 1984   1.8992 [1.2974;  2.7802]        6.7\nFredman et al., 1989  0.3831 [0.0521;  2.8146]        1.3\nMurphy et al., 1987   2.2733 [1.3248;  3.9011]        5.8\nPenninx et al., 1999  1.6163 [0.8944;  2.9208]        5.5\nPulska et al., 1998   2.9547 [1.4041;  6.2177]        4.7\nRoberts et al., 1990  2.5683 [2.0855;  3.1629]        7.5\nSaz et al., 1999      2.8278 [1.7510;  4.5666]        6.1\nSharma et al., 1998   2.5949 [1.1555;  5.8275]        4.3\nTakeida et al., 1997  7.7354 [4.4252; 13.5219]        5.7\nTakeida et al., 1999  6.7101 [4.3351; 10.3862]        6.4\nThomas et al., 1992   1.3555 [0.7623;  2.4104]        5.6\nThomas et al., 1992   1.8472 [1.1082;  3.0791]        6.0\nWeissman et al., 1986 1.3158 [0.6013;  2.8793]        4.5\nZheng et al., 1997    2.0324 [1.4110;  2.9275]        6.8\n\nNumber of studies: k = 18\nNumber of observations: o = 94770\nNumber of events: e = 5439\n\n                         OR           95%-CI    t  p-value\nRandom effects model 2.2901 [1.7512; 2.9949] 6.52 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.2032 [0.0744; 0.6314]; tau = 0.4508 [0.2728; 0.7946]\n I^2 = 72.9% [56.7%; 83.0%]; H = 1.92 [1.52; 2.43]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 62.73   17 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)"
  },
  {
    "objectID": "05-between_study_heterogeneity.html",
    "href": "05-between_study_heterogeneity.html",
    "title": "5  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "06-forest_plots.html",
    "href": "06-forest_plots.html",
    "title": "6  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "07-subgroup_analyses.html",
    "href": "07-subgroup_analyses.html",
    "title": "7  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "08-meta_regression.html",
    "href": "08-meta_regression.html",
    "title": "8  Meta-Regressie",
    "section": "",
    "text": "In meta-regressie passen we conventionele regressietechnieken aan op gegevens op studieniveau. Subgroepanalyses kunnen worden gezien als een speciaal geval van meta-regressie met categorische voorspellers en een gemeenschappelijke schatting van \\(\\tau^2\\). Het doel van een meta-regressiemodel is om (delen van) de ware verschillen in effectgrootte in onze data te verklaren (d.w.z. de heterogeniteitsvariantie tussen studies, \\(\\tau^2\\)). Als een model goed bij de gegevens past, moet de afwijking van de ware effecten van de regressielijn kleiner zijn dan hun oorspronkelijke afwijking van het gepoolde effect. Als dit het geval is, zal de onverklaarde of residuele heterogeniteit klein zijn. Dit wordt weergegeven door de R2∗index, die ons vertelt welk percentage van de heterogeniteitsvariatie door ons model wordt verklaard.\nBij meervoudige meta-regressie worden twee of meer voorspellers gebruikt in hetzelfde meta-regressiemodel. Het is ook mogelijk om te testen of de voorspellingen van een variabele veranderen voor verschillende waarden van een andere variabele, door interactietermen te introduceren. Hoewel (meervoudige) meta-regressie erg veelzijdig is, is het niet zonder beperkingen. Meervoudige meta-regressie maakt het erg gemakkelijk om modellen te overfitten, wat betekent dat willekeurige ruis in plaats van echte relaties gemodelleerd worden. Multi-collineariteit van voorspellers kan ook een bedreiging vormen voor de geldigheid van ons model. Er zijn verschillende manieren om ervoor te zorgen dat ons meta-regressiemodel robuust is. We kunnen bijvoorbeeld alleen modellen passen die gebaseerd zijn op een vooraf gedefinieerde theoretische rationale, of permutatietesten gebruiken. Multi-model inferentie kan worden gebruikt als een verkennende aanpak. Deze methode kan ons wijzen op mogelijk belangrijke voorspellers en kan worden gebruikt om hypotheses af te leiden die in toekomstig onderzoek moeten worden getest."
  },
  {
    "objectID": "09-publication_bias.html",
    "href": "09-publication_bias.html",
    "title": "9  Publicatie Bias",
    "section": "",
    "text": "Publicatiebias treedt op wanneer sommige studies systematisch ontbreken in de gepubliceerde literatuur, en dus ook in onze meta-analyse. Strikt genomen is er sprake van publicatiebias als de waarschijnlijkheid dat een studie gepubliceerd wordt afhangt van de resultaten. Er is echter ook een reeks andere vertekeningen in de rapportage. Deze vertekeningen beïnvloeden ook hoe waarschijnlijk het is dat een bevinding in onze meta-analyse terechtkomt. Voorbeelden zijn citatiebias, taalbias of uitkomstbias. Het is ook mogelijk dat gepubliceerd bewijs vertekend is, bijvoorbeeld door twijfelachtige onderzoekspraktijken. Twee veelvoorkomende van dat soort praktijken zijn \\(p\\)-hacking en HARKing, en beide kunnen het risico op overschatting van effecten in een meta-analyse vergroten.\nVeel methoden voor publicatiebias zijn gebaseerd op het idee van kleine-studie-effecten. Deze benaderingen gaan ervan uit dat alleen kleine studies met een verrassend hoge effectgrootte significante resultaten behalen en daarom worden geselecteerd voor publicatie. Dit leidt tot een asymmetrische funnel plot, wat een teken kan zijn van publicatiebias. Maar dat hoeft niet zo te zijn. Er zijn ook verschillende “goedaardige” oorzaken van effecten van kleine studies mogelijk. Een relatief nieuwe methode, p-curve, is gebaseerd op het idee dat we kunnen controleren op bewijskracht door alleen maar te kijken naar het patroon van significante (p&lt;0,05) effecten in onze gegevens. Deze methode kan worden gebruikt om te testen op zowel de aan- als afwezigheid van een echt effect en kan de grootte ervan schatten.\nEr is geen bewijs dat een bepaalde publicatiebias beter veel beter is dan een ander. Maar wat moet je dan kiezen? Om dit probleem aan te pakken, raden we aan om altijd meerdere methoden te gebruiken bij het evalueren van publicatiebias. Hieronder vind je een overzicht van de meest gebruikte methoden met de voor- en nadelen erbij.\n\n\n\nMethoden om de ware effectgrootte te schatten, gecorrigeerd voor publicatiebias: Overzicht van voor- en nadelen.\n\n\n\nVoordelen\nNadelen\n\n\n\n\nDuval & Tweedie Trim-and-Fill\nVery heavily used in practice. Can be interpreted by many researchers.\nOften fails to correct the effect size enough, for example when the true effect is zero. Not robust when the heterogeneity is very large; often outperformed by other methods.\n\n\nPET-PEESE\nBased on a simple and intuitive model. Easy to implement and interpret.\nSometimes massively over- or underestimates the effect. Weak performance for meta-analyses with few studies, low sample sizes, and high heterogeneity.\n\n\nLimit Meta-Analysis\nSimilar approach as PET-PEESE, but explicitly models between-study heterogeneity.\nPerformance is less well studied than the one of other methods. May fail when the number of studies is very low (&lt;10) and heterogeneity very high.\n\n\nP-Curve\nHas been shown to outperform other methods (particularly trim-and-fill) when its assumptions are met.\nWorks under the assumption of no heterogeneity, which is unlikely in practice. Requires a minimum number of significant effect sizes. Less easy to interpret and communicate.\n\n\nSelection Models\nCan potentially model any kind of assumed selection process. The three-parameter selection model has shown good performance in simulation studies.\nOnly valid when the selection model describes the publication bias process adequately. Assume that other small-study effects are not relevant. Can be difficult to interpret and requires background knowledge.\n\n\n\n\n\n\n\nHet is vaak moeilijk, zo niet onmogelijk, om te weten welke aanpak het meest geschikt is voor onze gegevens en of de resultaten betrouwbaar zijn. Zoals we al eerder zeiden, zal de exacte mate waarin selectieve rapportage onze resultaten heeft beïnvloed altijd onbekend zijn. Maar door verschillende publicatiebias technieken toe te passen, kunnen we iets produceren dat lijkt op een reeks geloofwaardige ware effecten.\nSelectiemodellen zijn een zeer veelzijdige methode en kunnen worden gebruikt om verschillende publicatiebiasprocessen te modelleren. Ze geven echter alleen geldige resultaten als het veronderstelde model adequaat is en vereisen vaak een zeer groot aantal onderzoeken. Een zeer eenvoudig selectiemodel, het drieparametermodel, kan ook worden gebruikt voor kleinere datasets. Geen enkele publicatiebiasmethode presteert consistent beter dan alle andere. Het is daarom raadzaam om altijd meerdere technieken toe te passen en de gecorrigeerde effectgrootte voorzichtig te interpreteren. Grondig zoeken naar ongepubliceerd bewijs vermindert het risico van publicatiebias veel beter dan de huidige statistische benaderingen."
  },
  {
    "objectID": "10-multilevel_meta-analysis.html",
    "href": "10-multilevel_meta-analysis.html",
    "title": "10  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "11-seq_meta-analysis.html",
    "href": "11-seq_meta-analysis.html",
    "title": "11  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "12-network_meta-analysis.html",
    "href": "12-network_meta-analysis.html",
    "title": "12  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "13-bayesian_meta-analysis.html",
    "href": "13-bayesian_meta-analysis.html",
    "title": "13  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "14-power_analysis.html",
    "href": "14-power_analysis.html",
    "title": "14  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "15-risk_bias_plots.html",
    "href": "15-risk_bias_plots.html",
    "title": "15  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "16-reporting_reproducibility.html",
    "href": "16-reporting_reproducibility.html",
    "title": "16  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "17-es_calculation_conversion.html",
    "href": "17-es_calculation_conversion.html",
    "title": "17  Introductie",
    "section": "",
    "text": "With unprecedented amounts of research findings produced each day, it is even more important to view and critically appraise bodies of evidence in their entirety. Meta-analysis can be enormously helpful in achieving this, as long as we acknowledge its own limitations and biases.\nThe aim of meta-analysis is to combine, summarize and interpret all available evidence pertaining to a clearly defined research field or research question. However, it is only one method to do this. There are at least three distinct ways through which evidence from multiple studies can be synthesized\n- Traditional/narrative reviews (written by experts and authorities in the field without clear rules).\n- Systematic reviews (to summarize evidence using clearly defined and transparent rules).\n- Meta-analyses (to combine and summarize evidence across all studies using clearly defined and transparent rules in quantitative way).\nMeta-analysis was not invented by one person alone, but by many founding mothers and fathers. The first attempts to statistically summarize the effects of separate, but similar studies date back around 100 years. Countless innovations have helped to increase the applicability, robustness and versatility of meta-analytic methods in the last four decades. In the last decades, meta-analysis has become a universally accepted research tool.\nThere are many pitfalls and the book leads us through a quick tour of common meta-analysis pitfalls:\n\nThe “Apples and Oranges” Problem: One may argue that meta-analysis means combining apples with oranges. The scope and specificity of a meta-analysis should therefore be based on the research question it wants to answer, and this question should be of practical relevance. Variation between studies can often be unproblematic, and even insightful if it is correctly incorporated into the aims and problem specification of a meta-analysis.\nThe “Garbage In, Garbage Out” Problem: The quality of evidence produced by a meta-analysis heavily depends on the quality of the studies it summarizes. Otherwise you get “garbage in, garbage out”.\nThe “File Drawer” Problem: The file drawer problem refers to the problem that not all relevant research findings are published. Or negatieve or ‘disappointing’ results are underrepresented. You have to look at publication bias\nThe “Researcher Agenda” Problem: Meta-analysis comes with many “researcher degrees of freedom”, leaving much space for decisions which may sometimes be arbitrary, and sometimes the result of undisclosed personal preferences. One way to reduce the researcher agenda problem is pre-registration, and publishing a detailed analysis plan before beginning with the data collection for a meta-analysis.\n\nImportant is that the methodological decisions are both transparent and reproducible. Here the first building blocks are presented. 1. Define the research question: o define a good research question, it helps to first see it as a form of problem specification. Step by step, asking yourself these questions should make it easier to define what you want to achieve with your meta-analysis. Use e.g. FINER framework (Feasible, Interesting, Novel, Ethical, Relevant) or PICO (Population, Intervention, Comparison, Outcome) to define your research question. Look also at eligible research designs, take into account cultural and linguistic range of eligble studies, and look at publication type and the role of grey literature. Write these things in inclusion and exclusion criteria. For all this take into account the PRISMA () and MARS ()-guidelines.\nMany of these problems can be mitigated by defining a clear research question and eligibility criteria, writing an analysis plan, pre-registering the meta-analysis, and conducting the study search and data extraction in a systematic and reproducible way."
  },
  {
    "objectID": "02-discovering_R.html#theory",
    "href": "02-discovering_R.html#theory",
    "title": "2  Discovering R",
    "section": "2.1 Theory",
    "text": "2.1 Theory\nR has become one of the most powerful and frequently used statistical programming languages in the world. Whether you are working in academia or at a company, the things you can do in R will often seem like a superpower to others. But it is a superpower that everyone can learn, provided some time and effort. The book shows how to install R and RStudio (the last one which gives us a user interface which makes it easier to handle our data, packages and output). R is not a computer program with a graphical user interface and pre-defined functionality. It show how it works.\nR is a full programming language to which people all around the world can contribute freely available add-ons, so-called packages. Important packages for working with this book and to do meta-analysis are meta, metafor, dmetar and (for data wrangling) tidyverse. You have to install these on your computer. When installed you have to load them everytime you start your work.\nThe fundamental building blocks of R are functions. Many of these functions can be imported through packages which we can install from the internet. Functions can be used to import, manipulate, transform and save data using R. Examples important for doing meta-analysis in R are given."
  },
  {
    "objectID": "02-discovering_R.html#practice",
    "href": "02-discovering_R.html#practice",
    "title": "2  Discovering R",
    "section": "2.2 Practice",
    "text": "2.2 Practice\nInstall first the packages needed for this book. I had to install an other version of meta to install the package dmetar.\n\n#require(remotes)\n#install_version(\"meta\", version = \"6.5.0\", repos = \"http://cran.us.r-project.org\")\n\n#install.packages(\"tidyverse\")\n\n#install.packages(\"metafor\")\n\n#install.packages(\"devtools\")\n\n#devtools::install_github(\"MathiasHarrer/dmetar\")\n\nLoad the packages needed for this book.\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(metafor)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: metadat\nLoading required package: numDeriv\n\nLoading the 'metafor' package (version 4.4-0). For an\nintroduction to the package please type: help(metafor)\n\nlibrary(dmetar)\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\n\nDownload the example dataset SuicidePrevention.xlx from here and open it.\n\nlibrary(openxlsx)\ndata &lt;-read.xlsx(\"SuicidePrevention.xlsx\")\n\nLet us have a look at the dataset and look at the kind of variables we have.\n\nglimpse(SuicidePrevention)\n\nRows: 9\nColumns: 10\n$ author    &lt;chr&gt; \"Berry et al.\", \"DeVries et al.\", \"Fleming et al.\", \"Hunt & …\n$ n.e       &lt;dbl&gt; 90, 77, 30, 64, 50, 109, 60, 40, 51\n$ mean.e    &lt;dbl&gt; 14.98, 16.21, 3.01, 19.32, 4.54, 15.11, 3.44, 7.10, 23.74\n$ sd.e      &lt;dbl&gt; 3.29, 5.35, 0.87, 6.41, 2.75, 4.63, 1.26, 0.76, 7.24\n$ n.c       &lt;dbl&gt; 95, 69, 30, 65, 50, 111, 60, 40, 56\n$ mean.c    &lt;dbl&gt; 15.54, 20.13, 3.13, 20.22, 5.61, 16.46, 3.42, 7.38, 24.91\n$ sd.c      &lt;dbl&gt; 4.41, 7.43, 1.23, 7.62, 2.66, 5.39, 1.88, 1.41, 10.65\n$ pubyear   &lt;dbl&gt; 2006, 2019, 2006, 2011, 1997, 2000, 2013, 2015, 2014\n$ age_group &lt;fct&gt; gen, older, gen, gen, gen, gen, gen, older, older\n$ control   &lt;fct&gt; WLC, no intervention, no intervention, WLC, WLC, no interven…\n\n\nYou can check specfic columns directly\n\nSuicidePrevention$n.e\n\n[1]  90  77  30  64  50 109  60  40  51\n\n\nAnd look what kind of variable is this:\n\nclass(SuicidePrevention$n.e)\n\n[1] \"numeric\"\n\n\nLook at the mean of this variable.\n\nmean(SuicidePrevention$n.e)\n\n[1] 63.44444\n\n\nBe sure the variables have the good class, e.g.here numeric. Let us change this for some variables.\n\nSuicidePrevention$mean.e &lt;- as.numeric(SuicidePrevention$mean.e)\nSuicidePrevention$sd.e &lt;- as.numeric(SuicidePrevention$sd.e)\nSuicidePrevention$n.c &lt;- as.numeric(SuicidePrevention$n.c)\nSuicidePrevention$mean.c &lt;- as.numeric(SuicidePrevention$mean.c)\nSuicidePrevention$sd.c &lt;- as.numeric(SuicidePrevention$sd.c)\nSuicidePrevention$n.c &lt;- as.numeric(SuicidePrevention$n.c)\n\nTwo variables we have to change to factors to analyse them correctly.\n\nSuicidePrevention$age_group &lt;- as.factor(SuicidePrevention$age_group)\nSuicidePrevention$control &lt;- as.factor(SuicidePrevention$control)\n\nIn my case the variable age_group is already split in the levels gen and older, see here:\n\nSuicidePrevention$age_group\n\n[1] gen   older gen   gen   gen   gen   gen   older older\nLevels: gen older\n\n\nA logical variable can also be handy, e.g. for the next one:\n\nSuicidePrevention$pubyear\n\n[1] 2006 2019 2006 2011 1997 2000 2013 2015 2014\n\n\n\nas.logical(SuicidePrevention$pubyear &gt;= 2010)\n\n[1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nSometimes it is good to look at specific data from the dataframe. Remember: data.frame[rows, columns]. So look at row 2 becomes\n\nSuicidePrevention[2,]\n\n          author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n2 DeVries et al.  77  16.21 5.35  69  20.13 7.43    2019     older\n          control\n2 no intervention\n\n\nIf you want to see information of first column and second row becomes\n\nSuicidePrevention[2,1]\n\n[1] \"DeVries et al.\"\n\n\nFor selecting parts of the dataframe use c() function. To extract rows 2 and 3 as well as columns 4 and 6 becomes\n\nSuicidePrevention[c(2,3),c(4,6)]\n\n  sd.e mean.c\n2 5.35  20.13\n3 0.87   3.13\n\n\nRows you select by number, columns you also can select by name,a s in this example.\n\nSuicidePrevention[, c(\"author\", \"control\")]\n\n           author         control\n1    Berry et al.             WLC\n2  DeVries et al. no intervention\n3  Fleming et al. no intervention\n4    Hunt & Burke             WLC\n5 McCarthy et al.             WLC\n6   Meijer et al. no intervention\n7   Rivera et al. no intervention\n8  Watkins et al. no intervention\n9  Zaytsev et al. no intervention\n\n\nYou can also filter a data set based on row values.\nFor example (more examples as given):\n\nfilter(SuicidePrevention, n.e &lt;= 50)\n\n           author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n1  Fleming et al.  30   3.01 0.87  30   3.13 1.23    2006       gen\n2 McCarthy et al.  50   4.54 2.75  50   5.61 2.66    1997       gen\n3  Watkins et al.  40   7.10 0.76  40   7.38 1.41    2015     older\n          control\n1 no intervention\n2             WLC\n3 no intervention\n\n\nYou can transform data easy. So, if you want to transform the study of De Vries et al. from 2019 to 2018 you can write it as:\n\nSuicidePrevention[2, \"pubyear\"] &lt;- 2018\nSuicidePrevention[2, \"pubyear\"]\n\n[1] 2018\n\n\nIf you want to add a new column to your dataframe (e.g mean difference) you can do it as follows:\n\nSuicidePrevention$md &lt;- SuicidePrevention$mean.e - SuicidePrevention$mean.c\n\nOnce we transformed it, we can save it. You can save it as a .rda file\n\nsave(SuicidePrevention, file = \"SuicidePrevention.rda\")\n\nor as a .csv file\n\nwrite.csv(SuicidePrevention, file = \"SuicidePrevention.csv\")\n\nImportant: Keep your transformation syntax, so you know what you did from original data to the final data."
  },
  {
    "objectID": "03-effect_sizes.html#theory",
    "href": "03-effect_sizes.html#theory",
    "title": "3  Effect Sizes",
    "section": "3.1 Theory",
    "text": "3.1 Theory\nEffect size is defined here as a metric quantifying the relationship between two entities. It captures the direction and magnitude of this relationship. If relationships are expressed as the same effect size, it is possible to compare them.Effect sizes are the building blocks of meta-analyses.\nTo perform a meta-analysis, we need at least an estimate of the effect size and its standard error. They should be comparable, computable, reliable and interpretable. The standard error of an effect size represents how precise the study’s estimate of the effect is. Meta-analysis gives effect sizes with a greater precision a higher weight because they are better estimators of the true effect.\nThere are various effect sizes we can use in meta-analyses. Common ones are “one-variable” relationship measures, such as: - means (summing all individual values deviding by sum of sample size);\n- proportions (number of individuals falling into a specific subgroup deviding by total number of individuals);\n- correlations (expresses teh amount of co-variation between two variables, for example the Pearson Product-Moment Correlation and Point-Biserial Correlation);\n- (standardized) mean differences (difference in means between two independent groups deviding by the pooled standard deviation); - risk ratio (relative risk) as the ratio of two risks; - odds ratio, as the number of cases which fall into a specific subgroup deviding by the number of cases which do not fall into that subgroup. - incidence rate ratios (also called rate ratio), takes into account the person/time-aspects of two groups.\nEffect sizes can also be biased, for example by measurement error and range restriction. There are formulas to correct for some biases, including the small sample bias of standardized mean differences, attenuation due to unreliability, as well as range restriction problems. Other common problems are that studies report the data needed to calculate effect sizes in different formats, as well as the unit-of-analysis problem, which arises when studies contribute more than one effect size."
  },
  {
    "objectID": "03-effect_sizes.html#practice",
    "href": "03-effect_sizes.html#practice",
    "title": "3  Effect Sizes",
    "section": "3.2 Practice",
    "text": "3.2 Practice\nLet us look first at the bewteen-group standardized mean difference (SMD) effect size. It is used when the outcome is continuous and the predictor is categorical. It is the difference in means between two independent groups deviding by the pooled standard deviation. It is also called Cohen’s d.\n\\[SMD_{between} = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p}\\]\nwhere \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the means of the two groups and \\(s_p\\) is the pooled standard deviation. SMD’s are often interpreted as small (0.2), medium (0.5) and large (0.8) effects.\nThere are several functions in R which allow us to calculate SMDbetween/Cohen’s d in one step. Here, we use the esc_mean_sd function, which is part of the esc-package. We have not used this package before, so it is necessary to install it first.\n\n# install.packages(\"esc\")\nlibrary(esc)\n\n# Define the data we need to calculate SMD/d\n# This is just some example data that we made up\ngrp1m &lt;- 50   # mean of group 1\ngrp2m &lt;- 60   # mean of group 2\ngrp1sd &lt;- 10  # sd of group 1\ngrp2sd &lt;- 10  # sd of group 2\ngrp1n &lt;- 100  # n of group1\ngrp2n &lt;- 100  # n of group2\n\n# Calculate effect size\nesc_mean_sd(grp1m = grp1m, grp2m = grp2m, \n            grp1sd = grp1sd, grp2sd = grp2sd, \n            grp1n = grp1n, grp2n = grp2n)\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: mean and sd to effect size d\n    Effect Size:  -1.0000\n Standard Error:   0.1500\n       Variance:   0.0225\n       Lower CI:  -1.2940\n       Upper CI:  -0.7060\n         Weight:  44.4444\n\n\nTo conduct a meta-analysis of standardized mean differences, our data set should at least contain the following columns:\n\nn.e. The number of observations in the intervention/experimental group.\nmean.e. The mean of the intervention/experimental group.\nsd.e. The standard deviation in the intervention/experimental group.\nn.c. The number of observations in the control group.\nmean.c. The mean of the control group.\nsd.c. The standard deviation in the control group.\n\nFor binary outcomes odds ratios can be used. The esc_2x2 function in the esc package provides an easy way to calculate the (log) odds ratio in R.\n\nlibrary(esc)\n\n# Define data\ngrp1yes &lt;- 45  # events in the treatment group\ngrp1no &lt;- 98   # non-events in the treatment group\ngrp2yes &lt;- 67  # events in the control group\ngrp2no &lt;- 76   # non-events in the control group\n\n# Calculate OR by setting es.type to \"or\"\nesc_2x2(grp1yes = grp1yes, grp1no = grp1no,\n        grp2yes = grp2yes, grp2no = grp2no,\n        es.type = \"or\")\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: 2x2 table (OR) coefficient to effect size odds ratio\n    Effect Size:   0.5209\n Standard Error:   0.2460\n       Variance:   0.0605\n       Lower CI:   0.3216\n       Upper CI:   0.8435\n         Weight:  16.5263\n\n\nTo conduct a meta-analysis of odds ratios in R, the following columns should be included in our data set:\n\nevent.e. The number of events in the treatment or experimental group.\n\nn.e. The sample size of the treatment or experimental group.\n\nevent.c. The number of events in the control group.\n\nn.c. The sample size of the control group.\n\nFor a practice sample for correction we look at small sample correction. We can easily convert unstandardized SMDs/Cohen’s dto Hedges’ g using the hedges_g function in the esc-package.\n\n# Load esc package\nlibrary(esc)\n\n# Define uncorrected SMD and sample size n\nSMD &lt;- 0.5\nn &lt;- 30\n\n# Convert to Hedges g\ng &lt;- hedges_g(SMD, n)\ng\n\n[1] 0.4864865"
  },
  {
    "objectID": "02-discovering_R.html#theorie",
    "href": "02-discovering_R.html#theorie",
    "title": "2  R ontdekken",
    "section": "2.1 Theorie",
    "text": "2.1 Theorie\nR is uitgegroeid tot een van de krachtigste en meest gebruikte statistische programmeertalen ter wereld. Of je nu in de academische wereld werkt of in een bedrijf, de dingen die je in R kunt doen lijken vaak een superkracht voor anderen. Maar het is een superkracht die iedereen kan leren, mits je er wat tijd en moeite in stot.\nHet boek laat zien hoe je R en RStudio installeert (de laatste geeft ons een gebruikersinterface die het makkelijker maakt om met onze gegevens, pakketten en uitvoer om te gaan). R is geen computerprogramma met een grafische gebruikersinterface en voorgedefinieerde functionaliteit. RStudio laat jou makkelijker met R werken. R is de moter en RStduio is meer het dashboard. Het boek laat zien hoe het met elkaar werkt.\nR is een volledige programmeertaal waaraan mensen over de hele wereld met vrij beschikbare uitbreidingen, zogenaamde specifieke pakketten, kunnen bijdragen. Belangrijke specifieke pakketten voor het werken met dit boek en het doen van meta-analyse met R zijn meta (zie bv hier), metafor(zie bv hier), dmetar (zie hier) en (voor data bewerking) tidyverse (zie hier.\nNadat je R en RStudio hebt geïnstalleerd op jouw computer, moet deze pakketten installeren op je computer. De fundamentele bouwstenen van R zijn functies, waarmee in dit hoofdstuk wordt gewerkt en waarmee je een goed idee krijgt hoe een en ander voor het uitvoeren van meta-analyses werkt. Veel van de functies kunnen worden geïmporteerd en uitgevoerd via pakketten die met internet zijn te installeren. Functies kunnen worden gebruikt om gegevens te importeren, te manipuleren, te transformeren en op te slaan met behulp van R. Er worden voorbeelden gegeven van basisfuncties die belangrijk zijn voor het uitvoeren van meta-analyses in R en in de hoofdstukken hierna worden uitgevoerd."
  },
  {
    "objectID": "02-discovering_R.html#practijk",
    "href": "02-discovering_R.html#practijk",
    "title": "2  Discovering R",
    "section": "2.2 Practijk",
    "text": "2.2 Practijk\nInstalleer eerst de pakketten die nodig zijn voor dit boek. Ik moest zelf een vorige versie van meta installeren om het pakket dmetar te installeren. Als je dit niet eerder hebt gedaan met je de hekjes (#) weghalen en de code uitvoeren. Dat installeren hoef je maar één keer te doen.\n\n#require(remotes)\n#install_version(\"meta\", version = \"6.5.0\", repos = \"http://cran.us.r-project.org\")\n\n#install.packages(\"tidyverse\")\n\n#install.packages(\"metafor\")\n\n#install.packages(\"devtools\")\n\n#devtools::install_github(\"MathiasHarrer/dmetar\")\n\nLaad de pakketten die nodig zijn om de voorbeelden in dit boek uit te voeren.\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\nlibrary(metafor)\n\nLoading required package: Matrix\n\n\nLoading required package: metadat\n\n\nLoading required package: numDeriv\n\n\n\nLoading the 'metafor' package (version 4.4-0). For an\nintroduction to the package please type: help(metafor)\n\nlibrary(dmetar)\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nHaal ook de datasetSuicidePrevention.xlx die je hier vandaan kunt halen en open die set.\n\nlibrary(openxlsx)\ndata &lt;-read.xlsx(\"SuicidePrevention.xlsx\")\n\nLaten we naar die dataset kijken en zien welke variabelen er zijn. Er zijn tien kolommen (variabelen) en negen rijgen (individuele studies)\n\nglimpse(SuicidePrevention)\n\nRows: 9\nColumns: 10\n$ author    &lt;chr&gt; \"Berry et al.\", \"DeVries et al.\", \"Fleming et al.\", \"Hunt & …\n$ n.e       &lt;dbl&gt; 90, 77, 30, 64, 50, 109, 60, 40, 51\n$ mean.e    &lt;dbl&gt; 14.98, 16.21, 3.01, 19.32, 4.54, 15.11, 3.44, 7.10, 23.74\n$ sd.e      &lt;dbl&gt; 3.29, 5.35, 0.87, 6.41, 2.75, 4.63, 1.26, 0.76, 7.24\n$ n.c       &lt;dbl&gt; 95, 69, 30, 65, 50, 111, 60, 40, 56\n$ mean.c    &lt;dbl&gt; 15.54, 20.13, 3.13, 20.22, 5.61, 16.46, 3.42, 7.38, 24.91\n$ sd.c      &lt;dbl&gt; 4.41, 7.43, 1.23, 7.62, 2.66, 5.39, 1.88, 1.41, 10.65\n$ pubyear   &lt;dbl&gt; 2006, 2019, 2006, 2011, 1997, 2000, 2013, 2015, 2014\n$ age_group &lt;fct&gt; gen, older, gen, gen, gen, gen, gen, older, older\n$ control   &lt;fct&gt; WLC, no intervention, no intervention, WLC, WLC, no interven…\n\n\nJe kunt de inhoud van een bepaalde kolommen alleen bekijken, bv deze:\n\nSuicidePrevention$n.e\n\n[1]  90  77  30  64  50 109  60  40  51\n\n\nEn je kunt ook zien wat voor een soort variabele het is.\n\nclass(SuicidePrevention$n.e)\n\n[1] \"numeric\"\n\n\nLaten we het gemiddelde van deze variabele vaststellen.\n\nmean(SuicidePrevention$n.e)\n\n[1] 63.44444\n\n\nZorg ervoor dat de variabelen de goede klasse hebben, bijvoorbeeld numeriek. Laten we dit veranderen voor enkele variabelen.\n\nSuicidePrevention$mean.e &lt;- as.numeric(SuicidePrevention$mean.e)\nSuicidePrevention$sd.e &lt;- as.numeric(SuicidePrevention$sd.e)\nSuicidePrevention$n.c &lt;- as.numeric(SuicidePrevention$n.c)\nSuicidePrevention$mean.c &lt;- as.numeric(SuicidePrevention$mean.c)\nSuicidePrevention$sd.c &lt;- as.numeric(SuicidePrevention$sd.c)\nSuicidePrevention$n.c &lt;- as.numeric(SuicidePrevention$n.c)\n\nTwee variabelen moeten we veranderen in factoren om de analyse later correct te kunnen uitvoeren.\n\nSuicidePrevention$age_group &lt;- as.factor(SuicidePrevention$age_group)\nSuicidePrevention$control &lt;- as.factor(SuicidePrevention$control)\n\nIn dit geval is de variabele age_group opgesplitst in de niveaus gen en older, zie hier:\n\nSuicidePrevention$age_group\n\n[1] gen   older gen   gen   gen   gen   gen   older older\nLevels: gen older\n\n\nEen logische variabele kan ook handig zijn, bijvoorbeeld voor de volgende variabele:\n\nSuicidePrevention$pubyear\n\n[1] 2006 2019 2006 2011 1997 2000 2013 2015 2014\n\n\nDeze zetten we om in een logische variabele.\n\nas.logical(SuicidePrevention$pubyear &gt;= 2010)\n\n[1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nSoms is het goed om naar specifieke gegevens uit het dataframe te kijken. Onthoud: data.frame[rijen, kolommen]. Dus kijken naar rij 2 wordt:\n\nSuicidePrevention[2,]\n\n          author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n2 DeVries et al.  77  16.21 5.35  69  20.13 7.43    2019     older\n          control\n2 no intervention\n\n\nAls je informatie van de eerste kolom én de tweede rij wilt zien, schrijf je:\n\nSuicidePrevention[2,1]\n\n[1] \"DeVries et al.\"\n\n\nGebruik de functie c() om delen van het dataframe te selecteren. Om rijen 2 en 3 en kolommen 4 en 6 te extraheren wordt\n\nSuicidePrevention[c(2,3),c(4,6)]\n\n  sd.e mean.c\n2 5.35  20.13\n3 0.87   3.13\n\n\nRijen selecteer je op nummer, kolommen kun je ook op naam selecteren, zoals in dit voorbeeld.\n\nSuicidePrevention[, c(\"author\", \"control\")]\n\n           author         control\n1    Berry et al.             WLC\n2  DeVries et al. no intervention\n3  Fleming et al. no intervention\n4    Hunt & Burke             WLC\n5 McCarthy et al.             WLC\n6   Meijer et al. no intervention\n7   Rivera et al. no intervention\n8  Watkins et al. no intervention\n9  Zaytsev et al. no intervention\n\n\nJe kunt voor een gegevensverzameling ook de functie filter gebruiken, die werkt op basis van rijwaarden, bijvoorbeeld de rijwaarden van n.e. zijn kleiner of gelijk aan 50. Dan schrijf je:\n\nfilter(SuicidePrevention, n.e &lt;= 50)\n\n           author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n1  Fleming et al.  30   3.01 0.87  30   3.13 1.23    2006       gen\n2 McCarthy et al.  50   4.54 2.75  50   5.61 2.66    1997       gen\n3  Watkins et al.  40   7.10 0.76  40   7.38 1.41    2015     older\n          control\n1 no intervention\n2             WLC\n3 no intervention\n\n\nJe kunt gegevens zelf gemakkelijk transformeren. Dus als je de studie van De Vries et al. wilt transformeren van 2019 naar 2018, kun je het schrijven als:\n\nSuicidePrevention[2, \"pubyear\"] &lt;- 2018\nSuicidePrevention[2, \"pubyear\"]\n\n[1] 2018\n\n\nAls je een nieuwe kolom wilt toevoegen aan je dataframe (bijvoorbeeld het gemiddelde verschil), kun je dat als volgt doen:\n\nSuicidePrevention$md &lt;- SuicidePrevention$mean.e - SuicidePrevention$mean.c\n\nZodra alles is getransformeerd, kunnen we het bewerkte bestand opslaan. Je kunt het opslaan als een .rda-bestand, zo dan:\n\nsave(SuicidePrevention, file = \"SuicidePrevention.rda\")\n\nof opslaan als een .csv-bestand, zo dan:\n\nwrite.csv(SuicidePrevention, file = \"SuicidePrevention.csv\")\n\n\nBewaar je transformatie syntax, zodat je weet wat je hebt gedaan van de originele gegevens naar de uiteindelijke gegevens."
  },
  {
    "objectID": "02-discovering_R.html#praktijk",
    "href": "02-discovering_R.html#praktijk",
    "title": "2  R ontdekken",
    "section": "2.2 Praktijk",
    "text": "2.2 Praktijk\nInstalleer eerst de pakketten die nodig zijn voor dit boek. Ik moest zelf een vorige versie van meta installeren om het pakket dmetar te installeren. Als je dit niet eerder hebt gedaan met je de hekjes (#) weghalen en de code uitvoeren. Dat installeren hoef je maar één keer te doen.\n\n#require(remotes)\n#install_version(\"meta\", version = \"6.5.0\", repos = \"http://cran.us.r-project.org\")\n\n#install.packages(\"tidyverse\")\n\n#install.packages(\"metafor\")\n\n#install.packages(\"devtools\")\n\n#devtools::install_github(\"MathiasHarrer/dmetar\")\n\nLaad de pakketten die nodig zijn om de voorbeelden in dit boek uit te voeren.\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\nlibrary(metafor)\n\nLoading required package: Matrix\n\n\nLoading required package: metadat\n\n\nLoading required package: numDeriv\n\n\n\nLoading the 'metafor' package (version 4.4-0). For an\nintroduction to the package please type: help(metafor)\n\nlibrary(dmetar)\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nHaal ook de datasetSuicidePrevention.xlx die je hier vandaan kunt halen en open die set.\n\nlibrary(openxlsx)\ndata &lt;-read.xlsx(\"SuicidePrevention.xlsx\")\n\nLaten we naar die dataset kijken en zien welke variabelen er zijn. Er zijn tien kolommen (variabelen) en negen rijgen (individuele studies)\n\nglimpse(SuicidePrevention)\n\nRows: 9\nColumns: 10\n$ author    &lt;chr&gt; \"Berry et al.\", \"DeVries et al.\", \"Fleming et al.\", \"Hunt & …\n$ n.e       &lt;dbl&gt; 90, 77, 30, 64, 50, 109, 60, 40, 51\n$ mean.e    &lt;dbl&gt; 14.98, 16.21, 3.01, 19.32, 4.54, 15.11, 3.44, 7.10, 23.74\n$ sd.e      &lt;dbl&gt; 3.29, 5.35, 0.87, 6.41, 2.75, 4.63, 1.26, 0.76, 7.24\n$ n.c       &lt;dbl&gt; 95, 69, 30, 65, 50, 111, 60, 40, 56\n$ mean.c    &lt;dbl&gt; 15.54, 20.13, 3.13, 20.22, 5.61, 16.46, 3.42, 7.38, 24.91\n$ sd.c      &lt;dbl&gt; 4.41, 7.43, 1.23, 7.62, 2.66, 5.39, 1.88, 1.41, 10.65\n$ pubyear   &lt;dbl&gt; 2006, 2019, 2006, 2011, 1997, 2000, 2013, 2015, 2014\n$ age_group &lt;fct&gt; gen, older, gen, gen, gen, gen, gen, older, older\n$ control   &lt;fct&gt; WLC, no intervention, no intervention, WLC, WLC, no interven…\n\n\nJe kunt de inhoud van een bepaalde kolommen alleen bekijken, bv deze:\n\nSuicidePrevention$n.e\n\n[1]  90  77  30  64  50 109  60  40  51\n\n\nEn je kunt ook zien wat voor een soort variabele het is.\n\nclass(SuicidePrevention$n.e)\n\n[1] \"numeric\"\n\n\nLaten we het gemiddelde van deze variabele vaststellen.\n\nmean(SuicidePrevention$n.e)\n\n[1] 63.44444\n\n\nZorg ervoor dat de variabelen de goede klasse hebben, bijvoorbeeld numeriek. Laten we dit veranderen voor enkele variabelen.\n\nSuicidePrevention$mean.e &lt;- as.numeric(SuicidePrevention$mean.e)\nSuicidePrevention$sd.e &lt;- as.numeric(SuicidePrevention$sd.e)\nSuicidePrevention$n.c &lt;- as.numeric(SuicidePrevention$n.c)\nSuicidePrevention$mean.c &lt;- as.numeric(SuicidePrevention$mean.c)\nSuicidePrevention$sd.c &lt;- as.numeric(SuicidePrevention$sd.c)\nSuicidePrevention$n.c &lt;- as.numeric(SuicidePrevention$n.c)\n\nTwee variabelen moeten we veranderen in factoren om de analyse later correct te kunnen uitvoeren.\n\nSuicidePrevention$age_group &lt;- as.factor(SuicidePrevention$age_group)\nSuicidePrevention$control &lt;- as.factor(SuicidePrevention$control)\n\nIn dit geval is de variabele age_group opgesplitst in de niveaus gen en older, zie hier:\n\nSuicidePrevention$age_group\n\n[1] gen   older gen   gen   gen   gen   gen   older older\nLevels: gen older\n\n\nEen logische variabele kan ook handig zijn, bijvoorbeeld voor de volgende variabele:\n\nSuicidePrevention$pubyear\n\n[1] 2006 2019 2006 2011 1997 2000 2013 2015 2014\n\n\nDeze zetten we om in een logische variabele.\n\nas.logical(SuicidePrevention$pubyear &gt;= 2010)\n\n[1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nSoms is het goed om naar specifieke gegevens uit het dataframe te kijken. Onthoud: data.frame[rijen, kolommen]. Dus kijken naar rij 2 wordt:\n\nSuicidePrevention[2,]\n\n          author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n2 DeVries et al.  77  16.21 5.35  69  20.13 7.43    2019     older\n          control\n2 no intervention\n\n\nAls je informatie van de eerste kolom én de tweede rij wilt zien, schrijf je:\n\nSuicidePrevention[2,1]\n\n[1] \"DeVries et al.\"\n\n\nGebruik de functie c() om delen van het dataframe te selecteren. Om rijen 2 en 3 en kolommen 4 en 6 te extraheren wordt\n\nSuicidePrevention[c(2,3),c(4,6)]\n\n  sd.e mean.c\n2 5.35  20.13\n3 0.87   3.13\n\n\nRijen selecteer je op nummer, kolommen kun je ook op naam selecteren, zoals in dit voorbeeld.\n\nSuicidePrevention[, c(\"author\", \"control\")]\n\n           author         control\n1    Berry et al.             WLC\n2  DeVries et al. no intervention\n3  Fleming et al. no intervention\n4    Hunt & Burke             WLC\n5 McCarthy et al.             WLC\n6   Meijer et al. no intervention\n7   Rivera et al. no intervention\n8  Watkins et al. no intervention\n9  Zaytsev et al. no intervention\n\n\nJe kunt voor een gegevensverzameling ook de functie filter gebruiken, die werkt op basis van rijwaarden, bijvoorbeeld de rijwaarden van n.e. zijn kleiner of gelijk aan 50. Dan schrijf je:\n\nfilter(SuicidePrevention, n.e &lt;= 50)\n\n           author n.e mean.e sd.e n.c mean.c sd.c pubyear age_group\n1  Fleming et al.  30   3.01 0.87  30   3.13 1.23    2006       gen\n2 McCarthy et al.  50   4.54 2.75  50   5.61 2.66    1997       gen\n3  Watkins et al.  40   7.10 0.76  40   7.38 1.41    2015     older\n          control\n1 no intervention\n2             WLC\n3 no intervention\n\n\nJe kunt gegevens zelf gemakkelijk transformeren. Dus als je de studie van De Vries et al. wilt transformeren van 2019 naar 2018, kun je het schrijven als:\n\nSuicidePrevention[2, \"pubyear\"] &lt;- 2018\nSuicidePrevention[2, \"pubyear\"]\n\n[1] 2018\n\n\nAls je een nieuwe kolom wilt toevoegen aan je dataframe (bijvoorbeeld het gemiddelde verschil), kun je dat als volgt doen:\n\nSuicidePrevention$md &lt;- SuicidePrevention$mean.e - SuicidePrevention$mean.c\n\nZodra alles is getransformeerd, kunnen we het bewerkte bestand opslaan. Je kunt het opslaan als een .rda-bestand, zo dan:\n\nsave(SuicidePrevention, file = \"SuicidePrevention.rda\")\n\nof opslaan als een .csv-bestand, zo dan:\n\nwrite.csv(SuicidePrevention, file = \"SuicidePrevention.csv\")\n\n\n\n\n\n\n\nImportant\n\n\n\nBewaar je transformatie syntax, zodat je weet wat je hebt gedaan van de originele gegevens naar de uiteindelijke gegevens."
  },
  {
    "objectID": "03-effect_sizes.html#theorie",
    "href": "03-effect_sizes.html#theorie",
    "title": "3  Effect Sizes",
    "section": "3.1 Theorie",
    "text": "3.1 Theorie\nEffect size wordt hier gedefinieerd als een metriek die de relatie tussen twee entiteiten kwantificeert. Het geeft de richting en sterkte van deze relatie weer. Als relaties worden uitgedrukt in een vergelijkbare effect size, is het mogelijk om ze te vergelijken. Effect sizes zijn de bouwstenen van meta-analyses.Om een meta-analyse uit te voeren, is op zijn minst een schatting nodig van de effect size en de standaard-fout ervan. Deze moeten vergelijkbaar, berekenbaar, betrouwbaar en interpreteerbaar zijn. De standaard-fout van een effect size geeft aan hoe nauwkeurig de schatting van het effect van de studie is. Meta-analyse geeft effect-sizes met een grotere precisie een hoger gewicht omdat ze betere schatters zijn van het ware effect.\nEr zijn verschillende effectmaten die we kunnen gebruiken in meta-analyses. Gebruikelijke zijn “één-variabele” relatiematen, zoals:\n- gemiddelden (som van alle individuele waarden, gedeeld door de som van de steekproefgrootte);\n- proporties (aantal individuen dat in een specifieke subgroep valt, gedeeld door het totale aantal individuen);\n- correlaties (drukt de mate van co-variatie tussen twee variabelen uit, bijvoorbeeld de Pearson Product-Moment Correlatie en Point-Biserial Correlatie);\n- risicoverhouding (relatief risico) als de verhouding tussen twee risico’s;\n- odds ratio, als het aantal gevallen dat in een bepaalde subgroep valt, gedeeld door het aantal gevallen dat niet in die subgroep valt;\n- incidentieratio’s (ook wel rate ratio genoemd), houdt rekening met de persoon/tijd-aspecten van twee groepen.\nEffectgroottes kunnen ook vertekend zijn, bijvoorbeeld door meetfouten en bereikbeperking. Er bestaan formules om te corrigeren voor sommige vertekeningen, waaronder de kleine steekproefvertekening van gestandaardiseerde gemiddelde verschillen, verzwakking door onbetrouwbaarheid en problemen met bereikbeperking. Andere veel voorkomende problemen zijn dat studies de gegevens die nodig zijn om effectgroottes te berekenen in verschillende formaten rapporteren, evenals het probleem van de analyse-eenheid, dat optreedt wanneer studies meer dan één effect size bijdragen."
  },
  {
    "objectID": "03-effect_sizes.html#praktijk",
    "href": "03-effect_sizes.html#praktijk",
    "title": "3  Effect Sizes",
    "section": "3.2 Praktijk",
    "text": "3.2 Praktijk\nLaten we eerst kijken naar de gestandaardiseerde size van het verschil tussen groepen (SMD). Deze wordt gebruikt als de uitkomst continu is en de voorspeller categorisch. Het is het verschil in gemiddelden tussen twee onafhankelijke groepen gedeeld door de gepoolde standaardafwijking. Het wordt ook Cohen’s \\(d\\) genoemd.\n\\[SMD_{between} = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p}\\]\nwaarbij \\(\\bar{X}_1\\) en \\(\\bar{X}_2\\) de gemiddelden van de twee groepen zijn en \\(s_p\\) de gepoolde standaardafwijking. SMD’s worden vaak geïnterpreteerd als kleine (0,2), middelgrote (0,5) en grote (0,8) effecten. Er zijn verschillende functies in R waarmee we SMDbetween/Cohen’s d in één stap kunnen berekenen. Hier gebruiken we de functie esc_mean_sd, die deel uitmaakt van het esc-pakket. We hebben dit pakket nog niet eerder gebruikt, dus het is noodzakelijk om het eerst te installeren.\n\n# install.packages(\"esc\")\nlibrary(esc)\n\n# Defineeer data die nodig zijn om SMD/d te berekenen\n# Dit is een eenvoudig voorbeeld om een dataset te maken\ngrp1m &lt;- 50   # gemiddelde van groep 1\ngrp2m &lt;- 60   # gemiddelde van groep 2\ngrp1sd &lt;- 10  # sd van groep 1\ngrp2sd &lt;- 10  # sd van groep 2\ngrp1n &lt;- 100  # n van groep 1 \ngrp2n &lt;- 100  # n van groep 2\n\n# CalculEER effect size\nesc_mean_sd(grp1m = grp1m, grp2m = grp2m, \n            grp1sd = grp1sd, grp2sd = grp2sd, \n            grp1n = grp1n, grp2n = grp2n)\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: mean and sd to effect size d\n    Effect Size:  -1.0000\n Standard Error:   0.1500\n       Variance:   0.0225\n       Lower CI:  -1.2940\n       Upper CI:  -0.7060\n         Weight:  44.4444\n\n\nOm een meta-analyse van gestandaardiseerde gemiddelde verschillen uit te voeren, moet onze dataset ten minste de volgende kolommen bevatten:\n\nn.e. Het aantal observaties in de interventie/experimentele groep.\n\nmean.e. Het gemiddelde van de interventie/experimentele groep.\n\nsd.e. De standaardafwijking in de interventie/experimentele groep.\n\nn.c. Het aantal observaties in de controlegroep.\n\nmean.c. Het gemiddelde van de controlegroep.\n\nsd.c. De standaardafwijking in de controlegroep.\n\nVoor binaire uitkomsten kunnen kansverhoudingen worden gebruikt. De functie esc_2x2 in het pakket esc biedt een eenvoudige manier om de (log)odds ratio in R te berekenen.\n\nlibrary(esc)\n\n# Defineeer data die nodig zijn om de odds ratio te berekenen\ngrp1yes &lt;- 45  # gebeurtenissen in de behandelingsgroep\ngrp1no &lt;- 98   # niet-gebeurtenissen in de behandelingsgroep\ngrp2yes &lt;- 67  # gebeurtenissen in de controlegroep\ngrp2no &lt;- 76   # niet-gebeurtenissen in de controlegroep\n\n# Calculeer OR door es.type op \"or\" te zetten\nesc_2x2(grp1yes = grp1yes, grp1no = grp1no,\n        grp2yes = grp2yes, grp2no = grp2no,\n        es.type = \"or\")\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: 2x2 table (OR) coefficient to effect size odds ratio\n    Effect Size:   0.5209\n Standard Error:   0.2460\n       Variance:   0.0605\n       Lower CI:   0.3216\n       Upper CI:   0.8435\n         Weight:  16.5263\n\n\nOm een meta-analyse van odds ratio’s uit te voeren in R, moeten de volgende kolommen worden opgenomen in onze dataset:\n\nevent.e. Het aantal geberutenissen in de experimentele groep.\nn.e. De sample size van de experimentele groep.\n\nevent.c. Het aantal gebeurtenissen in de controle groep.\n\nn.c. De sample size van de controle groep.\n\nVoor een voorbeeld voor correctie kijken we naar kleine steekproefcorrectie. We kunnen niet-gestandaardiseerde SMD’s/Cohen’s d eenvoudig omzetten naar Hedges’ g met behulp van de hedges_g functie in het esc pakket.\n\n# Laad esc pakket\nlibrary(esc)\n\n# Defineer niet aangepast SMD en sample size n\nSMD &lt;- 0.5\nn &lt;- 30\n\n# Zet om naar Hedges g\ng &lt;- hedges_g(SMD, n)\ng\n\n[1] 0.4864865"
  },
  {
    "objectID": "05-between_study_heterogeneity.html#theorie",
    "href": "05-between_study_heterogeneity.html#theorie",
    "title": "5  Heterogeniteit tussen studies",
    "section": "5.1 Theorie",
    "text": "5.1 Theorie\nIn meta-analyses moeten we niet alleen letten op de gepoolde (samen gebrachte) effectgrootte, maar ook op de heterogeniteit van de gegevens waarop dit gemiddelde effect is gebaseerd. Het totale effect legt niet vast dat de werkelijke effecten in sommige onderzoeken aanzienlijk kunnen verschillen van de puntschatting (bijv. subgroepen kunnen hele verschillende uitkomsten laten zien). Elke goede meta-analyse moet niet alleen een globaal overall-effect rapporteren, maar ook aangeven hoe betrouwbaar deze schatting is. Een essentieel onderdeel hiervan is het kwantificeren en analyseren van de heterogeniteit tussen studies \\(\\zeta_k\\) (anders dan de steekproeffout \\(\\epsilon_k\\)).\nCochran’s \\(Q\\) wordt vaak gebruikt om de variabiliteit in de meta-analyse gegevens te kwantificeren. Omdat we weten dat \\(Q\\) een verdeling van \\(\\chi^2\\) volgt, stelt deze maat ons in staat om te detecteren of er meer variatie aanwezig is dan verwacht kan worden op basis van alleen steekproeffouten. Deze overmatige variabiliteit bovenop de steekproeffout vertegenwoordigt echte verschillen in de effectgroottes van studies.\nEen statistische test van \\(Q\\) hangt echter sterk af van het type gegevens dat we hebben. We moeten niet alleen op \\(Q\\) vertrouwen om de mate van heterogeniteit te beoordelen. Er zijn andere maten, zoals \\(I^2\\), \\(H^2\\), \\(tau\\) of voorspellingsintervallen, die aanvullend gebruikt kunnen worden om iets over de tussen-studie-heterogeniteit te zeggen.\nHet gemiddelde effect in een meta-analyse kan vertekend zijn als er uitschieters in onze gegevens zitten. Uitschieters hebben niet altijd een grote invloed op de resultaten van een meta-analyse. Maar als ze dat wel doen, spreken we van invloedrijke gevallen. Er zijn verschillende methoden om uitschieters en invloedrijke gevallen te identificeren. Als dergelijke studies die eruit schieten worden ontdekt, is het raadzaam om de resultaten van de meta-analyse opnieuw te berekenen zonder deze studies om te zien of dit de interpretatie van onze resultaten verandert."
  },
  {
    "objectID": "05-between_study_heterogeneity.html#practice",
    "href": "05-between_study_heterogeneity.html#practice",
    "title": "5  Between-Study-Herterogeneity",
    "section": "5.2 Practice",
    "text": "5.2 Practice\nOok hier de onderstaande pakketten laden, het databestand ThirdWave openen en dit databestand goed bekijken.\n\nlibrary(tidyverse) # needed for 'glimpse'\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dmetar)\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\ndata(ThirdWave)\nglimpse(ThirdWave)\n\nRows: 18\nColumns: 8\n$ Author               &lt;chr&gt; \"Call et al.\", \"Cavanagh et al.\", \"DanitzOrsillo\"…\n$ TE                   &lt;dbl&gt; 0.7091362, 0.3548641, 1.7911700, 0.1824552, 0.421…\n$ seTE                 &lt;dbl&gt; 0.2608202, 0.1963624, 0.3455692, 0.1177874, 0.144…\n$ RiskOfBias           &lt;chr&gt; \"high\", \"low\", \"high\", \"low\", \"low\", \"low\", \"high…\n$ TypeControlGroup     &lt;chr&gt; \"WLC\", \"WLC\", \"WLC\", \"no intervention\", \"informat…\n$ InterventionDuration &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"sho…\n$ InterventionType     &lt;chr&gt; \"mindfulness\", \"mindfulness\", \"ACT\", \"mindfulness…\n$ ModeOfDelivery       &lt;chr&gt; \"group\", \"online\", \"group\", \"group\", \"online\", \"g…\n\n\nEr zitten acht variabelen (kolommen) in (Author, Year, TE, seTE, RiskOfBias, TypeControlGroup, InterventionDuration, InterventionType, and ModeOfDelivery) en achttien studies (rijen) in. De uitkomstmaat is TE (Treatment Effect) en de standaardfout van de uitkomstmaat is seTE (Standard Error of the Treatment Effect). De variabele Author bevat de namen van de auteurs van de studies. De uitkomstmaat is een Standardized Mean Difference (SMD), er wordt een random-effects model gebruikt met de methode REML.\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\nWe gaan het voorspellen.\n\nm.gen &lt;- update.meta(m.gen, prediction = TRUE)\n\nWe kunnen de resultaten van de meta-analyse bekijken met de summary functie.\n\nsummary(m.gen)\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.3\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.9\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.7\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies: k = 18\n\n                             SMD            95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [ 0.3782; 0.7760] 6.12 &lt; 0.0001\nPrediction interval              [-0.0572; 1.2115]              \n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)\n- Prediction interval based on t-distribution (df = 16)\n\n\nDit is wat er gerapporteerd kan worden: &gt; “De heterogeniteitsvariantie tussen studies werd geschat op \\(\\tau^2\\) = 0,08 (95%CI: 0,03-0,35), met een \\(I^2\\)-waarde van 63% (95%CI: 38-78%). Het voorspellingsinterval varieerde van \\(g\\) = -0,06 tot 1,21, wat aangeeft dat negatieve interventie-effecten niet kunnen worden uitgesloten voor toekomstige studies.”\nOm uitschieters te vinden hebben we de functie find.outliers gebruikt, die een object nodig heeft dat is gemaakt door de functie metagen.\n\nfind.outliers(m.gen)\n\nIdentified outliers (random-effects model) \n------------------------------------------ \n\"DanitzOrsillo\", \"Shapiro et al.\" \n \nResults with outliers removed \n----------------------------- \nReview:     Third Wave Psychotherapies\n\nNumber of studies: k = 16\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.4528 [0.3257; 0.5800] 7.59 &lt; 0.0001\nPrediction interval              [0.1687; 0.7369]              \n\nQuantifying heterogeneity:\n tau^2 = 0.0139 [0.0000; 0.1032]; tau = 0.1180 [0.0000; 0.3213]\n I^2 = 24.8% [0.0%; 58.7%]; H = 1.15 [1.00; 1.56]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 19.95   15  0.1739\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 15)\n- Prediction interval based on t-distribution (df = 14)\n\n\nWe zien dat de find.outliers functie twee uitschieters detecteerde, “DanitzOrsillo” en “Shapiro et al.”. Het {dmetar} pakket bevat ook een functie genaamd InfluenceAnalysis, waarmee we deze verschillende invloedsdiagnoses kunnen berekenen met behulp van één functie. De functie kan worden gebruikt voor elk type meta-analyseobject dat is gemaakt door {meta} functies. We slaan de resultaten van de functie op in een object genaamd m.gen.inf.\n\nm.gen.inf &lt;- InfluenceAnalysis(m.gen, random = TRUE)\n\n[===========================================================================] DONE \n\n\nDit creëert een baujat plot, die een grafische weergave geeft van de invloed van elke studie op de heterogeniteit van de meta-analyse.\n\nplot(m.gen.inf, \"baujat\")\n\nWarning: ggrepel: 12 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nDe volgende plot bevat verschillende invloedsdiagnoses voor elk van onze onderzoeken. Deze kunnen worden uitgezet met deze code:\n\nplot(m.gen.inf, \"influence\")\n\n\n\n\nHet {dmetar} pakket bevat, tenslotte, ook een functie genaamd InfluenceAnalysis, waarmee we deze resultaten kunnen berekenen met behulp van de ‘leave-one-out’-methode. De eerste is gesorteerd op effectgrootte, de tweede op heterogeniteit.\n\nplot(m.gen.inf, \"es\")\n\n\n\nplot(m.gen.inf, \"i2\")"
  },
  {
    "objectID": "10-multilevel_meta-analysis.html#theorie",
    "href": "10-multilevel_meta-analysis.html#theorie",
    "title": "10  “Multilevel” Meta-Analysis",
    "section": "10.1 Theorie",
    "text": "10.1 Theorie\nAls mensen het over multilevel meta-analyse hebben, denken ze aan meta-analyse modellen op drie niveaus. In Doing meta-analysis with R beschrijven Harrer et al. waarom meta-analyse van nature al een multilevel structuur van de data heeft en laten ze zien hoe zo’n conventionele meta-analyse kan worden uitbreid naar een model op drie niveaus. Aan de hand van een praktijkvoorbeeld laten ze het zien hoe het werkt in R.\nZe gaan eerst terug naar de random-effects model formule die ons bekend is: Deze formule beschrijft dus eigenlijk al een multilevelstructuur van meta-analysegegevens. Om dit duidelijker te maken, wordt de vergelijking opgesplitst in twee formules, waarbij elke formule correspondeert met een van de twee niveaus. Als we dit doen, krijgen we het volgende resultaat:\nNiveau 1 (participanten) model:\n\\[\\hat\\theta_k = \\theta_k + \\epsilon_k\\]\nNiveau 2 (studies) model:\n\\[\\theta_k = \\mu + \\zeta_k\\]\nDit oude-vertrouwde meta-analysemodel heeft de multilevel-eigenschap “ingebouwd”. Het heeft namelijk deze eigenschap omdat we aannemen dat deelnemers geclusterd zijn binnen studies in onze data. Het is nu mogelijk om deze twee-niveausstructuur verder uit te breiden om bepaalde mechanismen, die onze data hebben gegenereerd, beter vast te leggen. Dit is waar drie-niveaumodellen om de hoek komen kijken (Cheung 2014; Assink, Wibbelink, et al. 2016).\nModellen met drie niveaus kunnen worden gebruikt voor afhankelijke effectgroottes. Wanneer een bepaalde studie bijvoorbeeld meer dan één effect sizes omvat, kunnen we er meestal niet vanuit gaan dat deze verschillende resultaten onafhankelijk zijn. Een drie niveaumodel ondervangt dit probleem door aan te nemen dat effect sizes geclusterd zijn binnen grotere clusters (in dit geval bijvoorbeeld studies).\nDie model met drie niveau’s ziet er als volgt uit:\n\n\n\nDrie niveaustructuur\n\n\nHet is mogelijk om de formule van dit model met drie lagen op te schrijven met dezelfde formules die we hierboven gebruikten. Het grootste verschil is dat we nu drie formules moeten definiëren in plaats van twee:\nLevel 1 model:(participanten)\n\\[\\hat\\theta_{ij} = \\theta_{ij} + \\epsilon_{ij}\\]\nLevel 2 model:(binnen studies)\n\\[\\theta_{ij} = \\kappa_{j} + \\zeta_{(2)ij}\\]\nLevel 3 model:(tussen studies)\n\\[\\kappa_{j} = \\mu + \\zeta_{(3)j}\\]\nWaarbij \\(\\hat\\theta_{ij}\\) een schatting is van de ware effectgrootte \\(\\theta_{ij}\\). De term \\(ij\\) kan worden gelezen als “een effect size” \\(i\\) geclusterd in cluster \\(j\\)“. De parameter \\(\\kappa_{j}\\) is de gemiddelde effect size in cluster \\(j\\), en \\(\\mu\\) het totale gemiddelde populatie-effect. Net als voorheen kunnen we deze formules samenvoegen en zo de formule reduceren tot één regel:\n\\[\\hat\\theta_{ij} = \\mu + \\zeta_{(2)ij} + \\zeta_{(3)j} + \\epsilon_{ij}\\]\nIn tegenstelling tot de conventionele meta-analyse (random-effects models) schatten modellen met drie niveau’s twee heterogeniteitsvarianties: de random-effectvariantie binnen clusters en de heterogeniteitsvariantie tussen clusters (de clusters zijn in dit geval studies). Het is ook mogelijk om categorische of continue voorspellers te testen met een model met drie niveaus. Dit resulteert in een mixed-effects model met drie niveaus."
  },
  {
    "objectID": "10-multilevel_meta-analysis.html#praktijk",
    "href": "10-multilevel_meta-analysis.html#praktijk",
    "title": "10  “Multilevel” Meta-Analysis",
    "section": "10.2 Praktijk",
    "text": "10.2 Praktijk\n\n10.2.1 Voorbereiding\nHet {metafor} pakket is heel geschikt om meta-analyse op drie niveau’s uit te voeren. Het gebruikt ‘(restricted) maximum likelihood’ (REML-) procedures om dat te doen.\nWe moeten eerst de biblitheek hiervan openen.\n\n# Wees er zeker van dat het pakket is geïnstalleerd\nlibrary(metafor)\n\nVoor hier gebruiken we de Chernobyl data set. xxx Die dataset is deel van het dmetar pakket waar meer datasets voor meta-analyse in zitten. Laten we ook dat pakket, het pakket en de data set laden. Laten we meteen ook tidyverse laden, omdat we dat later nodig hebben.\n\n# Wees er zeker van dat het pakket is geïnstalleerd\nlibrary(dmetar)\nlibrary(tidyverse)\ndata(Chernobyl)\n\nLaten we eens zien hoe die dataset eruit ziet.\n\nhead(Chernobyl)\n\n                      author    cor   n         z       se.z       var.z\n1 Aghajanyan & Suskov (2009) 0.2061  91 0.2090949 0.10660036 0.011363636\n2 Aghajanyan & Suskov (2009) 0.2687  91 0.2754621 0.10660036 0.011363636\n3 Aghajanyan & Suskov (2009) 0.2049  92 0.2078420 0.10599979 0.011235955\n4 Aghajanyan & Suskov (2009) 0.2672  92 0.2738461 0.10599979 0.011235955\n5     Alexanin et al. (2010) 0.9317 559 1.6711230 0.04240945 0.001798561\n6     Alexanin et al. (2010) 0.4429 559 0.4758327 0.04240945 0.001798561\n  radiation es.id\n1       low  id_1\n2       low  id_2\n3       low  id_3\n4       low  id_4\n5       low  id_5\n6       low  id_6\n\nglimpse(Chernobyl)\n\nRows: 33\nColumns: 8\n$ author    &lt;chr&gt; \"Aghajanyan & Suskov (2009)\", \"Aghajanyan & Suskov (2009)\", …\n$ cor       &lt;dbl&gt; 0.2061, 0.2687, 0.2049, 0.2672, 0.9317, 0.4429, 0.8996, 0.95…\n$ n         &lt;dbl&gt; 91, 91, 92, 92, 559, 559, 559, 559, 559, 559, 560, 15, 79, 9…\n$ z         &lt;dbl&gt; 0.20909489, 0.27546213, 0.20784198, 0.27384610, 1.67112298, …\n$ se.z      &lt;dbl&gt; 0.10660036, 0.10660036, 0.10599979, 0.10599979, 0.04240945, …\n$ var.z     &lt;dbl&gt; 0.011363636, 0.011363636, 0.011235955, 0.011235955, 0.001798…\n$ radiation &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low…\n$ es.id     &lt;chr&gt; \"id_1\", \"id_2\", \"id_3\", \"id_4\", \"id_5\", \"id_6\", \"id_7\", \"id_…\n\n\nDe data set omvat acht kolommen (variabelen) en 33 rijen (studies). De eerste, author, geeft de naam van het onderzoek weer. De kolom cor toont de (niet-getransformeerde) correlatie tussen stralingsblootstelling en mutatiepercentages, terwijl n staat voor de steekproefgrootte. De kolommen z, se.z, en var.z zijn de Fisher-\\(z\\) getransformeerde correlaties, evenals hun standaardfout en variantie. De kolom radiation dient als moderator, die effectgroottes verdeelt in twee subgroepen, één met een lage en één met hoge totale stralingsblootstelling. De kolom es.id bevat eenvoudigweg een unieke ID voor elke effectgrootte (d.w.z. elke rij in ons dataframe).\nEen eigenaardigheid aan deze dataset is dat er herhaalde (geclusterde) vermeldingen in de kolom author staan. Dit komt doordat de meeste studies in deze meta-analyse meer dan één waargenomen effectgrootte hebben bijgedragen. Als we naar deze structuur kijken, is het duidelijk dat effectgroottes in onze dataset niet onafhankelijk zijn. Ze volgen een geclusterde structuur, waarbij verschillende effectgroottes geclusterd zijn in één studie. Het zou dus een goed idee kunnen zijn om een meta-analyse op drie niveaus uit te voeren om deze afhankelijkheden in onze gegevens adequaat te modelleren.\n\n\n10.2.2 Het fitten van een drie niveau ma-model\nEen meta-analysemodel met drie niveaus kan worden uitgevoerd met de functie rma.mv in {metafor}. Hier is een lijst van de belangrijkste argumenten voor deze functie en hoe ze gespecificeerd moeten worden:\n\n\nyi. De naam van de kolom in onze dataset die de berekende effectgroottes bevat. In ons voorbeeld is dit z, omdat Fisher-\\(z\\) getransformeerde correlaties betere wiskundige eigenschappen hebben dan “niet-getransformeerde” correlaties.\n\nV. De naam van de kolom in onze gegevensverzameling is die de variantie van de berekende effectgroottes bevat. In ons geval is dit var.z. Het is ook mogelijk om de gekwadrateerde standaardfout van de effectgrootte te gebruiken, aangezien \\(SE^2_k=vk\\)\n\ns.lab. De naam van de kolom in onze dataset die de studielabels bevat.\n\ndata. De naam van de data set.\n\ntest. De test die we willen toepassen op onze regressiecoëfficiënten. We kunnen kiezen uit \"z\" (standaard) en \"t\" (aanbevolen; gebruikt een test vergelijkbaar met de Knapp-Hartung methode).\n\nmethode. De methode die wordt gebruikt om de modelparameters te schatten. Zowel \"REML\" (aanbevolen; restricted maximum-likelihood) als \"ML\" (maximum likelihood) zijn mogelijk. Hou er rekening mee dat andere soorten heterogeniteitsschattingen tussen studies (bijv. Paule-Mandel) hier niet van toepassing zijn.\nrandom. De naam van de kolom die hier het belangrijkst is. de algemene structuur van de formule ziet er als volgt uit: ~ 1 | cluster/effecten_binnen_cluster. Hier gaan we uit van een structuur met drie niveaus: individuen (niveau 1), individuele effectgroottes (niveau 2) en studies (niveau 3), dus krijgen we de formule ~ 1 | author/es.id.\n\n\nfull.model &lt;- rma.mv(yi = z, \n                     V = var.z, \n                     slab = author,\n                     data = Chernobyl,\n                     random = ~ 1 | author/es.id, \n                     test = \"t\", \n                     method = \"REML\")\n\nDe resultaten van dit multilevel model kun je vervolgens krijgen door summary functie in te tikken:\n\nsummary(full.model)\n\n\nMultivariate Meta-Analysis Model (k = 33; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-21.1229   42.2458   48.2458   52.6430   49.1029   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed        factor \nsigma^2.1  0.1788  0.4229     14     no        author \nsigma^2.2  0.1194  0.3455     33     no  author/es.id \n\nTest for Heterogeneity:\nQ(df = 32) = 4195.8268, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval   ci.lb   ci.ub      \n  0.5231  0.1341  3.9008  32  0.0005  0.2500  0.7963  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nKijk naar deVariance Components:\n- sigma^2.1 toont de variantie op niveau 3 (variantie tussen clusters). Dit komt overeen met de heterogeniteitsvariantie tussen studies \\(tau^2\\) van een conventionele meta-analyse (aangezien de clusters author (14 stuks in totaal) in ons model studies vertegenwoordigen). Dit is gelijk aan 33 effectgroottes (author/es.id) en de heterogeniteitsvariantie binnen studies.\n- Onder Modelresultaten zien we de schatting van ons gepoolde effect (\\(z=\\) 0.52 (95%CI: 0.25-0.80)). Om de interpretatie te vergemakkelijken, is het raadzaam om het effect terug te transformeren naar een normale correlatie. Dit kan worden gedaan met de functie convert_z2r in het pakket {esc}:\n\nlibrary(esc)\nconvert_z2r(0.52)\n\n[1] 0.4777\n\n\nWe zien dat dit leidt tot een correlatie van ongeveer $\\(0,48. Dit kan als groot worden beschouwd. Er lijkt een substantieel verband te zijn tussen mutatiepercentages en blootstelling aan straling van Tsjernobyl. De `Test voor Heterogeniteit` in de uitvoer wijst op echte effectgrootteverschillen in onze gegevens (\\)p$&lt;0,001). Dit resultaat is echter niet erg informatief. We zijn meer geïnteresseerd in de precieze hoeveelheid heterogeniteitsvariantie die door elk niveau in ons model wordt gevangen. Het zou goed zijn om te weten hoeveel van de heterogeniteit wordt veroorzaakt door verschillen tussen participanten (niveau 1), binnen studies (niveau 2) en hoeveel door verschillen tussen studies (niveau 3).\n\n\n10.2.3 Variantie die aan elk niveau is toe te schrijven.\nIn modellen met drie niveaus wordt deze heterogeniteitsvariantie in drie delen gesplitst: individuele verschillen, een deel dat kan worden toegeschreven aan echte effectgrootteverschillen binnen clusters en een deel dat kan worden toegeschreven aan variatie tussen clusters. Er zijn dus drie \\(I^2\\) waarden, die het percentage van de totale variatie kwantificeren dat is geassocieerd met niveau 1, niveau 2 of niveau 3. De var.comp functie heeft alleen een passend rma.mv model als invoer nodig. We slaan de uitvoer op in i2 en gebruiken dan de functie summary om de resultaten af te drukken.\n\ni2 &lt;- var.comp(full.model)\nsummary(i2)\n\n        % of total variance    I2\nLevel 1            1.254966   ---\nLevel 2           39.525499 39.53\nLevel 3           59.219534 59.22\nTotal I2: 98.75% \n\n\nNo variance(niveau 1), binnen-variantie (niveau 2) is bijna \\(40\\%\\) en de tussen-variantie (level 3) is bijna \\(60\\%\\).\nWe kunnen dit visualiseren door het i2 object in de plot functie te zetten.\n\nplot(i2)\n\n\n\n\nIs een model met drie niveaus eigenlijk wel nodig? Hiervoor moeten we het drie-niveau model vergelijken met een twee-niveau model. Die vergelijking kunnen we doen met behulp van de functie anova. Laten we eerst een tweeniveaumodel maken. Dat doen we door onder method = \"REML\" de optie sigma2 = c(0, NA) toe te voegen. Dit betekent dat we de variantie op niveau 3 op 0 zetten.\n\nl3.removed &lt;- rma.mv(yi = z, \n                     V = var.z, \n                     slab = author,\n                     data = Chernobyl,\n                     random = ~ 1 | author/es.id, \n                     test = \"t\", \n                     method = \"REML\",\n                     sigma2 =  c(0, NA))\n\nsummary(l3.removed)\n\n\nMultivariate Meta-Analysis Model (k = 33; method: REML)\n\n  logLik  Deviance       AIC       BIC      AICc   \n-29.1742   58.3483   62.3483   65.2798   62.7621   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed        factor \nsigma^2.1  0.0000  0.0000     14    yes        author \nsigma^2.2  0.3550  0.5959     33     no  author/es.id \n\nTest for Heterogeneity:\nQ(df = 32) = 4195.8268, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    tval  df    pval   ci.lb   ci.ub      \n  0.5985  0.1051  5.6938  32  &lt;.0001  0.3844  0.8126  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNu is de functie anova te gebruiken door twee modellen te vergelijken. Dit geeft een \\(p\\)-waarde terug. Als nu de \\(p\\)-waarde significant is, heeft het complexere model (drie niveaus) de voorkeur. Als de \\(p\\)-waarde niet significant is, heeft het eenvoudigere model (twee niveaus) de voorkeur.\n\nanova(l3.removed, full.model)\n\n\n        df     AIC     BIC    AICc   logLik     LRT   pval        QE \nFull     3 48.2458 52.6430 49.1029 -21.1229                4195.8268 \nReduced  2 62.3483 65.2798 62.7621 -29.1742 16.1025 &lt;.0001 4195.8268 \n\n\nWe zien dat het Full (drie-niveaus) model inderdaad beter past dan het Reduced model met twee niveaus. De Akaike (AIC) en Bayesiaanse Informatie Criterium (BIC) zijn lager voor dit model, wat duidt op gunstige prestaties. De likelihood ratio test (LRT) die beide modellen vergelijkt is significant (\\(\\chi^2_1=\\) 16,1, \\(p&lt;\\) 0,001), en wijst dus in dezelfde richting.\nWe kunnen zeggen dat, hoewel het model met drie niveaus één extra parameter introduceert (d.w.z. het heeft 3 vrijheidsgraden in plaats van 2), deze extra complexiteit gerechtvaardigd lijkt. Het modelleren van de geclusterde datastructuur was waarschijnlijk een goed idee en heeft onze schatting van het gepoolde effect verbeterd.\nHoud er echter rekening mee dat er vaak goede redenen zijn om vast te houden aan een structuur met drie niveaus, zelfs als dit geen significant betere fit oplevert. Het is met name zinvol om vast te houden aan een model met drie niveaus als we denken dat het gebaseerd is op een solide theoretische onderbouwing.\n\n\n10.2.4 Subgroup analyse (moderatoren analyse) in dit drie niveau’s model\nZodra ons model op drie niveaus is ingesteld, is het ook mogelijk om de invloed van mogelijke moderatoren op het totale effect te beoordelen. Eerder in deze gids hebben we ontdekt dat subgroepanalyses kunnen worden uitgedrukt als een meta-regressiemodel met een dummycode voorspeller. Op vergelijkbare wijze kunnen we regressietermen toevoegen aan een “multilevel” model, wat leidt tot een ** mixed-effects model met drie niveaus**:\n\\[\\hat\\theta_{ij} = \\theta + \\beta x_i + \\zeta_{(2)ij} + \\zeta_{(3)j} + \\epsilon_{ij}\\]\nWaarbij \\(\\theta\\) het intercept is en \\(\\beta\\) het regressiegewicht van een voorspellende variabele \\(x\\). Als we \\(x_i\\) vervangen door een dummy, krijgen we een model dat kan worden gebruikt voor subgroepanalyses. Als \\(x\\) continu is, stelt de bovenstaande formule een meta-regressiemodel met drie niveaus voor.\nCategorische of continue voorspellers kunnen gespecificeerd worden in rma.mv met het mods argument. Het argument vereist een formule, beginnend met een tilde (~), en dan de naam van de voorspeller. Meervoudige meta-regressie is ook mogelijk door meer dan één voorspeller op te geven (bijvoorbeeld ~ var1 + var2).\nIn ons Chernobyl voorbeeld willen we controleren of correlaties verschillen afhankelijk van de totale hoeveelheid straling in het onderzochte monster (laag, gemiddeld of hoog). Deze informatie staat in de kolom `radiation`` in onze dataset. Met onderstaande code kunnen we een moderatorenmodel met drie niveaus fitten:\n\nmod.model &lt;- rma.mv(yi = z, V = var.z, \n                    slab = author, data = Chernobyl,\n                    random = ~ 1 | author/es.id, \n                    test = \"t\", method = \"REML\",\n                    mods = ~ radiation)\n\nsummary(mod.model)\n\nDe eerste belangrijke uitkomst is de Test van moderatoren. We zien dat \\(F_{2,28}\\)= 0,45, met \\(p\\)= 0,64. Dit betekent dat er geen significant verschil is tussen de subgroepen. Dit is hoe we de resultaten zouden beschrijven\n\n“De gepoolde correlatie op basis van het meta-analytische model met drie niveaus was \\(r=\\) 0,48 (95%CI: 0,25-0,66; \\(p\\) &lt; 0,001). De geschatte variantiecomponenten waren \\(0,179\\) en \\(0,119\\). Dit betekent dat \\(I^2_{Level 3}=\\) 0,179 en \\(I^2_{Level 2}=\\) 0,119. Dit betekent dat \\(I^2_{{Level 3}}=\\) 58,22% van de totale variatie kan worden toegeschreven aan tussen-cluster heterogeniteit en \\(I^2_{{Level 2}}=\\) 31,86% aan binnen-cluster heterogeniteit. We vonden dat het model met drie niveaus een significant betere fit gaf vergeleken met een model met twee niveaus waarbij heterogeniteit op niveau 3 tot nul werd beperkt (\\(I^2_1=\\) 16,10; \\(p\\)&lt; 0,001).”"
  },
  {
    "objectID": "06-forest_plots.html#theorie",
    "href": "06-forest_plots.html#theorie",
    "title": "6  De forest Plots",
    "section": "6.1 Theorie:",
    "text": "6.1 Theorie:\nHet is gebruikelijk om de resultaten van meta-analyses te visualiseren door middel van forest plots. Forest plots zijn op te vatten als een grafische weergave van de effectgrootte en het betrouwbaarheidsinterval van individuele studies en tonen ook het berekende totale effect. Ze laten het geobserveerde effect zien, het betrouwbaarheidsinterval en de gewichtsverdeling van elke studie. Ze laten ook het gepoolde effect zien dat berekend is over de studies.\nIn het Voorbeeld van een forest plot hieronder geeft een ruit het gemiddelde effect weer. De lengte van de ruit symboliseert het betrouwbaarheidsinterval van het gepoolde resultaat op de x-as. Meestal bevat een forest plot ook een verticale referentielijn die het punt op de x-as aangeeft dat gelijk is aan geen effect. In forest plots kun je ook iets laten zien over de heterogenities (zoals \\(I^2\\) of \\(\\tau^2\\) weer te geven.\n\n\n\nVoorbeeld van een forest plot\n\n\nHet is ook mogelijk om andere soorten informatie aan een forest plot toe te voegen, bijvoorbeeld de kwaliteitsbeoordeling die elke studie kreeg. Forest plots kunnen alleen resultaten weergeven die uitgaan van een vaste significantiedrempel, meestal \\(p&lt;0,05\\). Om te visualiseren hoe resultaten veranderen bij verschillende significantiedrempels, kunnen daarnaast draperieplots worden gegenereerd."
  },
  {
    "objectID": "07-subgroup_analyses.html#theorie",
    "href": "07-subgroup_analyses.html#theorie",
    "title": "7  Subgroep Analyse",
    "section": "7.1 Theorie",
    "text": "7.1 Theorie\nHoewel er verschillende manieren zijn om de heterogeniteit van een meta-analyse te beoordelen, vertellen die ons niet waarom we een overmatige variabiliteit in onze gegevens vinden. Subgroepanalyse stelt ons in staat om hypotheses te testen over waarom sommige onderzoeken een hogere of lagere effectgrootte hebben dan andere. We moeten verschillende studiekenmerken definiëren die de waargenomen effectgrootte kunnen beïnvloeden en elke studie dienovereenkomstig coderen.\nEr zijn talloze redenen waarom effectgroottes kunnen verschillen, maar we moeten ons beperken tot de redenen die van belang zijn in de context van onze analyse. Het idee achter subgroepanalyses is dat meta-analyse niet alleen gaat over het berekenen van een gemiddelde effectgrootte, maar dat het ook een hulpmiddel kan zijn om variatie in ons bewijs te onderzoeken. In subgroepanalyses zien we heterogeniteit niet alleen als hinderlijk, maar als interessante variatie die al dan niet verklaarbaar is door een wetenschappelijke hypothese. In het beste geval kan dit ons begrip van de wereld om ons heen vergroten, of op zijn minst praktische inzichten opleveren die richting geven aan toekomstige besluitvorming.\nVoor subgroepanalyses gaan we meestal uit van een fixed-effects model. Studies binnen subgroepen worden in de meeste gevallen gepoold met behulp van het random-effects model. Vervolgens wordt een \\(Q\\)-toets op basis van de algemene subgroepresultaten gebruikt om te bepalen of de groepen significant verschillen. Het subgroepanalysemodel wordt een “fixed-effects” model genoemd omdat de verschillende categorieën zelf als vast worden verondersteld. Dit betekent dat alle subgroepen worden verondersteld een gemeenschappelijke schatting van de heterogeniteit tussen de studies te delen.Ze vertegenwoordigen de enige waarden die de subgroepvariabele kan aannemen. Enkele voorbeelden van subgroepanalyses zijn: leeftijdsgroep, culturele achtergrond,controle-/interventiegroep, instrument om de uitkomst te meten, studiekwaliteit, soort, setting.\nBij het berekenen van een subgroepanalyse moeten we beslissen of afzonderlijke of gemeenschappelijke schattingen van de heterogeniteit tussen studies moeten worden gebruikt om de resultaten binnen subgroepen te poolen. Subgroepanalyses zijn geen wondermiddel en het belangrijk rekening te houden met de beperkingen en valkuilen van subgroepanalyses. Ze missen vaak de statistische power die nodig is om subgroepverschillen te detecteren. Daarom betekent een niet-significante test voor subgroepverschillen niet automatisch dat de subgroepen gelijkwaardige resultaten opleveren."
  },
  {
    "objectID": "05-between_study_heterogeneity.html#practijk",
    "href": "05-between_study_heterogeneity.html#practijk",
    "title": "5  Heterogeniteit tussen studies",
    "section": "5.2 Practijk",
    "text": "5.2 Practijk\nOok hier de onderstaande pakketten laden, het databestand ThirdWave openen en dit databestand goed bekijken.\n\nlibrary(tidyverse) # needed for 'glimpse'\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dmetar)\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\ndata(ThirdWave)\nglimpse(ThirdWave)\n\nRows: 18\nColumns: 8\n$ Author               &lt;chr&gt; \"Call et al.\", \"Cavanagh et al.\", \"DanitzOrsillo\"…\n$ TE                   &lt;dbl&gt; 0.7091362, 0.3548641, 1.7911700, 0.1824552, 0.421…\n$ seTE                 &lt;dbl&gt; 0.2608202, 0.1963624, 0.3455692, 0.1177874, 0.144…\n$ RiskOfBias           &lt;chr&gt; \"high\", \"low\", \"high\", \"low\", \"low\", \"low\", \"high…\n$ TypeControlGroup     &lt;chr&gt; \"WLC\", \"WLC\", \"WLC\", \"no intervention\", \"informat…\n$ InterventionDuration &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"sho…\n$ InterventionType     &lt;chr&gt; \"mindfulness\", \"mindfulness\", \"ACT\", \"mindfulness…\n$ ModeOfDelivery       &lt;chr&gt; \"group\", \"online\", \"group\", \"group\", \"online\", \"g…\n\n\nEr zitten acht variabelen (kolommen) in (Author, Year, TE, seTE, RiskOfBias, TypeControlGroup, InterventionDuration, InterventionType, and ModeOfDelivery) en achttien studies (rijen) in. De uitkomstmaat is TE (Treatment Effect) en de standaardfout van de uitkomstmaat is seTE (Standard Error of the Treatment Effect). De variabele Author bevat de namen van de auteurs van de studies. De uitkomstmaat is een Standardized Mean Difference (SMD), er wordt een random-effects model gebruikt met de methode REML.\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\nWe gaan het voorspellen.\n\nm.gen &lt;- update.meta(m.gen, prediction = TRUE)\n\nWe kunnen de resultaten van de meta-analyse bekijken met de summary functie.\n\nsummary(m.gen)\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.3\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.9\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.7\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies: k = 18\n\n                             SMD            95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [ 0.3782; 0.7760] 6.12 &lt; 0.0001\nPrediction interval              [-0.0572; 1.2115]              \n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)\n- Prediction interval based on t-distribution (df = 16)\n\n\nDit is wat er gerapporteerd kan worden:\n&gt; “De heterogeniteitsvariantie tussen studies werd geschat op \\(\\tau^2\\) = 0,08 (95%CI: 0,03-0,35), met een \\(I^2\\)-waarde van 63% (95%CI: 38-78%). Het voorspellingsinterval varieerde van \\(g\\) = -0,06 tot 1,21, wat aangeeft dat negatieve interventie-effecten niet kunnen worden uitgesloten voor toekomstige studies.”\nOm uitschieters te vinden hebben we de functie find.outliers gebruikt, die een object nodig heeft dat is gemaakt door de functie metagen.\n\nfind.outliers(m.gen)\n\nIdentified outliers (random-effects model) \n------------------------------------------ \n\"DanitzOrsillo\", \"Shapiro et al.\" \n \nResults with outliers removed \n----------------------------- \nReview:     Third Wave Psychotherapies\n\nNumber of studies: k = 16\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.4528 [0.3257; 0.5800] 7.59 &lt; 0.0001\nPrediction interval              [0.1687; 0.7369]              \n\nQuantifying heterogeneity:\n tau^2 = 0.0139 [0.0000; 0.1032]; tau = 0.1180 [0.0000; 0.3213]\n I^2 = 24.8% [0.0%; 58.7%]; H = 1.15 [1.00; 1.56]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 19.95   15  0.1739\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 15)\n- Prediction interval based on t-distribution (df = 14)\n\n\nWe zien dat de find.outliers functie twee uitschieters detecteerde, “DanitzOrsillo” en “Shapiro et al.”. Het {dmetar} pakket bevat ook een functie genaamd InfluenceAnalysis, waarmee we deze verschillende invloedsdiagnoses kunnen berekenen met behulp van één functie. De functie kan worden gebruikt voor elk type meta-analyseobject dat is gemaakt door {meta} functies. We slaan de resultaten van de functie op in een object genaamd m.gen.inf.\n\nm.gen.inf &lt;- InfluenceAnalysis(m.gen, random = TRUE)\n\n[===========================================================================] DONE \n\n\nDit creëert een baujat plot, die een grafische weergave geeft van de invloed van elke studie op de heterogeniteit van de meta-analyse.\n\nplot(m.gen.inf, \"baujat\")\n\nWarning: ggrepel: 12 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nDe volgende plot bevat verschillende invloedsdiagnoses voor elk van onze onderzoeken. Deze kunnen worden uitgezet met deze code:\n\nplot(m.gen.inf, \"influence\")\n\n\n\n\nHet {dmetar} pakket bevat, tenslotte, ook een functie genaamd InfluenceAnalysis, waarmee we deze resultaten kunnen berekenen met behulp van de ‘leave-one-out’-methode. De eerste is gesorteerd op effectgrootte, de tweede op heterogeniteit.\n\nplot(m.gen.inf, \"es\")\n\n\n\nplot(m.gen.inf, \"i2\")"
  },
  {
    "objectID": "06-forest_plots.html#praktijk",
    "href": "06-forest_plots.html#praktijk",
    "title": "6  De forest Plots",
    "section": "6.2 Praktijk:",
    "text": "6.2 Praktijk:\nHet meta-pakket in R is een veelgebruikte tool voor het maken van forest plots. Het heeft veel functies en de opmaak van de plots kan worden aangepast aan de wensen van de gebruiker.\nWe beginnen met het databestand zoals eerder omhoog gehaald.\n\nlibrary(tidyverse) # voor databewerking\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dmetar)    # voor de data\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(meta)      # voor de meta-analyse\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\ndata(ThirdWave)\nglimpse(ThirdWave)\n\nRows: 18\nColumns: 8\n$ Author               &lt;chr&gt; \"Call et al.\", \"Cavanagh et al.\", \"DanitzOrsillo\"…\n$ TE                   &lt;dbl&gt; 0.7091362, 0.3548641, 1.7911700, 0.1824552, 0.421…\n$ seTE                 &lt;dbl&gt; 0.2608202, 0.1963624, 0.3455692, 0.1177874, 0.144…\n$ RiskOfBias           &lt;chr&gt; \"high\", \"low\", \"high\", \"low\", \"low\", \"low\", \"high…\n$ TypeControlGroup     &lt;chr&gt; \"WLC\", \"WLC\", \"WLC\", \"no intervention\", \"informat…\n$ InterventionDuration &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"sho…\n$ InterventionType     &lt;chr&gt; \"mindfulness\", \"mindfulness\", \"ACT\", \"mindfulness…\n$ ModeOfDelivery       &lt;chr&gt; \"group\", \"online\", \"group\", \"group\", \"online\", \"g…\n\n\nm.gen bevat de resultaten van de meta-analyse. We kunnen de resultaten van de meta-analyse visualiseren met de forest()-functie.\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\nWe kunnen een forest plot maken voor elk type {meta} meta-analyse object (bijv. resultaten van metagen, metacont, of metabin) met behulp van de forest.meta-functie. We hoeven alleen maar forest.meta te voorzien van ons {meta} object en er wordt een plot gemaakt.\n\nforest.meta(m.gen, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE,\n            leftlabs = c(\"Author\", \"g\", \"SE\"))\n\n\n\n\nEr zijn allerlei argumenten in dit pakket om de forest plots verder aan te passen. Zo kunnen we de plot verbeteren door een kolom toe te voegen die het bias-risico van elke studie weergeeft. De dataset ThirdWave, die we gebruikten om m.gen te genereren, bevat een kolom met de naam RiskOfBias, waarin de beoordeling van het risico van vertekening van elke studie is opgeslagen.\nWe kunnen het argument leftcols gebruiken om de kolom aan de plot toe te voegen. Dit resulteert in de volgende code en de informatie van elke studie is toegevoegd.\n\nforest.meta(m.gen, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE,\n            leftcols = c(\"studlab\", \"TE\", \"seTE\", \"RiskOfBias\"),\n            leftlabs = c(\"Author\", \"g\", \"SE\", \"Risk of Bias\"))\n\n\n\n\nEr zijn ook twee voorgeprogrammeerde lay-outs voor forest plots:die van JAMA en van Cochrane. We kunnen de lay-out van de plot aanpassen met behulp van de layout-functie. Hier bijvoorbeeld de JAMA-lay-out.\n\nforest.meta(m.gen, layout = \"JAMA\")\n\n\n\n\nJe kunt de forest plots op verschillende manieren opslaan (bijvoorbeeld PDF, PNG, SVG). Hier slaan we de JAMA forest plot op als een pdf in onze img-folder.\n\npdf(\"img/forest_plot.pdf\")\nforest.meta(m.gen, layout = \"JAMA\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nBehalve de forest plot zijn er ook nog andere manieren op de resultaten te visualiseren (bv. draperie plots)."
  },
  {
    "objectID": "07-subgroup_analyses.html#praktijk",
    "href": "07-subgroup_analyses.html#praktijk",
    "title": "7  Subgroep Analyse",
    "section": "7.2 Praktijk",
    "text": "7.2 Praktijk\nHet uitvoeren van een subgroepanalyse met behulp van het {meta} pakket is relatief eenvoudig. In elke meta-analysefunctie in {meta} kan het argument subgroep worden gespecificeerd. Dit vertelt de functie welke effectgrootte in welke subgroep valt en voert een subgroepanalyse uit. Het argument subgroep accepteert diverse soorten variabelen. Het enige waar we op moeten letten is dat studies in dezelfde subgroep absoluut identieke labels hebben.\nLaten we het databestand ThirdWave gebruiken om een subgroepanalyse uit te voeren en kjken naar de kolommen author en RiskofBias.\n\nlibrary(tidyverse) # voor databewerking\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dmetar)    # voor de data\n\nExtensive documentation for the dmetar package can be found at: \n www.bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\nlibrary(meta)      # voor de meta-analyse\n\nLoading 'meta' package (version 6.5-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\ndata(ThirdWave)\nhead(ThirdWave[,c(\"Author\", \"RiskOfBias\")])\n\n           Author RiskOfBias\n1     Call et al.       high\n2 Cavanagh et al.        low\n3   DanitzOrsillo       high\n4  de Vibe et al.        low\n5  Frazier et al.        low\n6  Frogeli et al.        low\n\n\nLaten we eerst m.gen aanmaken en vervolgens de subgroepanalyse uitvoeren.\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\n\nupdate.meta(m.gen, \n            subgroup = RiskOfBias, \n            tau.common = FALSE)\n\nReview:     Third Wave Psychotherapies\n\nNumber of studies: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nResults for subgroups (random effects model (HK)):\n                    k    SMD           95%-CI  tau^2    tau     Q   I^2\nRiskOfBias = high   7 0.8126 [0.2835; 1.3417] 0.2423 0.4922 25.89 76.8%\nRiskOfBias = low   11 0.4300 [0.2770; 0.5830] 0.0099 0.0997 13.42 25.5%\n\nTest for subgroup differences (random effects model (HK)):\n                  Q d.f. p-value\nBetween groups 2.84    1  0.0917\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\nIn de uitvoer zien we een nieuw gedeelte genaamd Results for subgroups. Dit deel van de uitvoer toont de gepoolde effectgrootte afzonderlijk voor elke subgroep. We zien dat er k=7 studies zijn met een hoog risico op vertekening en 11 met een laag risico op vertekening. De geschatte heterogeniteit tussen studies verschilt aanzienlijk, met \\(I^2\\)=77% in studies met een hoog risico op vertekening, maar slechts 26% in studies met een laag risico. Met \\(g\\)=0,43 is de effectschatting in studies met een laag vertekeningsrisico kleiner dan in studies met een hoog vertekeningsrisico. Dit is een veel voorkomende bevinding, omdat vertekende studies de effecten van een behandeling eerder overschatten.\nMaar is het verschil statistisch significant? We kunnen dit controleren door te kijken naar de resultaten van de Test for subgroup differences. Dit toont ons de \\(Q\\)-test, die in ons voorbeeld met 2 subgroepen gebaseerd is op één vrijheidsgraad. De \\(p\\)-waarde van de test is 0,09, wat groter is dan de conventionele significantiedrempel, maar nog steeds een verschil op trendniveau aangeeft.We kunnen de resultaten ook controleren als we uitgaan van een gemeenschappelijke \\(\\tau^2\\)-schatting in beide subgroepen. We hoeven alleen tau.common op TRUE te zetten.\n\nupdate.meta(m.gen, subgroup = RiskOfBias, tau.common = TRUE)\n\nReview:     Third Wave Psychotherapies\n\nNumber of studies: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nQuantifying residual heterogeneity:\n tau^2 = 0.0691 [0.0208; 0.3268]; tau = 0.2630 [0.1441; 0.5717]\n I^2 = 59.3% [30.6%; 76.1%]; H = 1.57 [1.20; 2.05]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nResults for subgroups (random effects model (HK)):\n                    k    SMD           95%-CI  tau^2    tau     Q   I^2\nRiskOfBias = high   7 0.7691 [0.2533; 1.2848] 0.0691 0.2630 25.89 76.8%\nRiskOfBias = low   11 0.4698 [0.3015; 0.6382] 0.0691 0.2630 13.42 25.5%\n\nTest for subgroup differences (random effects model (HK)):\n                   Q d.f. p-value\nBetween groups  1.79    1  0.1814\nWithin groups  39.31   16  0.0010\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n  (assuming common tau^2 in subgroups)\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)\n\n\nWe zien dat de geschatte heterogeniteitsvariantie tussen studies \\(\\tau^2\\)= 0,069 is, en identiek in beide subgroepen. We hebben twee \\(Q\\)-tests: één tussen groepen (de eigenlijke subgroeptoets) en één voor de heterogeniteit binnen de subgroepen. Net als in een normale meta-analyse geeft dit laatste simpelweg aan dat er een overmatige variabiliteit is in de subgroepen (\\(p\\)= 0,001). De test van subgroepverschillen geeft opnieuw aan dat er geen significant verschil is tussen studies met een laag versus hoog risico op vertekening (p= 0,181).\nDe resultaten van subgroepanalyses worden meestal gerapporteerd in een tabel met het geschatte effect en de heterogeniteit in elke subgroep, evenals de p-waarde van de test voor subgroepverschillen.\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n\n\n\n$g$\n95\\%CI\n$p$\n$I^2$\n95\\%CI\n$p$subgroup\n\n\n\n\nRisk of Bias\n\n\n\n\n\n0.092\n\n\n- High\n0.81\n0.28-1.34\n0.009\n0.77\n0.51-0.89\n\n\n\n- Low\n0.43\n0.28-0.58\n$&lt;$ 0.001\n0.25\n0.00-0.63"
  }
]